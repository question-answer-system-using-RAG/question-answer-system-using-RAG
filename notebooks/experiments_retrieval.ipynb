{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Union, Optional\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import ndcg_score\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "from sentence_transformers import CrossEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Абстрактный класс для моделей эмбеддинга\n",
    "class EmbeddingModel(ABC):\n",
    "    @abstractmethod\n",
    "    def compute_embeddings(self, texts: List[str]) -> Union[List[List[float]], np.ndarray]:\n",
    "        \"\"\"Вычисляет эмбеддинги для списка текстов.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def compute_single_embedding(self, text: str) -> Union[List[float], np.ndarray]:\n",
    "        \"\"\"Вычисляет эмбеддинг для одного текста.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_vector_size(self) -> int:\n",
    "        \"\"\"Возвращает размерность векторов для модели.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_model_name(self) -> str:\n",
    "        \"\"\"Возвращает имя модели для отчетов.\"\"\"\n",
    "        pass\n",
    "\n",
    "# Реализация для SentenceTransformer\n",
    "class SentenceTransformerModel(EmbeddingModel):\n",
    "    def __init__(self, model_name: str = 'BAAI/bge-large-en-v1.5'):\n",
    "        self.model_name = model_name\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "    \n",
    "    def compute_embeddings(self, texts: List[str]) -> List[List[float]]:\n",
    "        embeddings = self.model.encode(texts, normalize_embeddings=True)\n",
    "        return [embedding.tolist() for embedding in embeddings]\n",
    "    \n",
    "    def compute_single_embedding(self, text: str) -> List[float]:\n",
    "        embedding = self.model.encode(text, normalize_embeddings=True)\n",
    "        return embedding.tolist()\n",
    "    \n",
    "    def get_vector_size(self) -> int:\n",
    "        return self.model.get_sentence_embedding_dimension()\n",
    "    \n",
    "    def get_model_name(self) -> str:\n",
    "        return f\"SentenceTransformer-{self.model_name.split('/')[-1]}\"\n",
    "\n",
    "# Реализация для BM25\n",
    "class BM25Model(EmbeddingModel):\n",
    "    def __init__(self):\n",
    "        self.corpus = None\n",
    "        self.bm25 = None\n",
    "        self.vector_size = 768  # Фиктивное значение для совместимости с Qdrant\n",
    "        # Подготовка для токенизации и стемминга\n",
    "        self.stemmer = PorterStemmer()\n",
    "        try:\n",
    "            nltk.data.find('tokenizers/punkt')\n",
    "            nltk.data.find('corpora/stopwords')\n",
    "        except LookupError:\n",
    "            nltk.download('punkt')\n",
    "            nltk.download('stopwords')\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def preprocess_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Предобработка текста: токенизация, удаление стоп-слов, стемминг.\"\"\"\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        tokens = [self.stemmer.stem(token) for token in tokens if token.isalnum() and token not in self.stop_words]\n",
    "        return tokens\n",
    "    \n",
    "    def initialize_corpus(self, texts: List[str]) -> None:\n",
    "        \"\"\"Инициализация индекса BM25 на основе корпуса документов.\"\"\"\n",
    "        processed_corpus = [self.preprocess_text(text) for text in texts]\n",
    "        self.corpus = texts  # Сохраняем оригинальные тексты\n",
    "        self.bm25 = BM25Okapi(processed_corpus)\n",
    "    \n",
    "    def compute_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Для BM25 возвращаем фиктивные эмбеддинги.\n",
    "        В реальном использовании будем напрямую использовать BM25 для поиска.\n",
    "        \"\"\"\n",
    "        if self.corpus is None:\n",
    "            self.initialize_corpus(texts)\n",
    "        \n",
    "        # Возвращаем фиктивные эмбеддинги для совместимости с API\n",
    "        return np.random.rand(len(texts), self.vector_size)\n",
    "    \n",
    "    def compute_single_embedding(self, text: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Для BM25 эмбеддинг не используется.\n",
    "        Возвращаем фиктивный эмбеддинг для совместимости с API.\n",
    "        \"\"\"\n",
    "        return np.random.rand(self.vector_size)\n",
    "    \n",
    "    def get_vector_size(self) -> int:\n",
    "        return self.vector_size\n",
    "    \n",
    "    def get_model_name(self) -> str:\n",
    "        return \"BM25\"\n",
    "    \n",
    "    def search(self, query: str, limit: int = 6) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Поиск схожих документов с помощью BM25.\"\"\"\n",
    "        if self.bm25 is None:\n",
    "            raise ValueError(\"BM25 не инициализирован. Сначала вызовите compute_embeddings с корпусом документов.\")\n",
    "        \n",
    "        query_tokens = self.preprocess_text(query)\n",
    "        scores = self.bm25.get_scores(query_tokens)\n",
    "        \n",
    "        # Получаем индексы документов, отсортированных по убыванию релевантности\n",
    "        top_indices = np.argsort(scores)[::-1][:limit]\n",
    "        \n",
    "        # Формируем результаты в том же формате, что и для других моделей\n",
    "        results = []\n",
    "        for i in top_indices:\n",
    "            if scores[i] > 0:  # Добавляем только документы с ненулевой релевантностью\n",
    "                results.append({\n",
    "                    'id': i,\n",
    "                    'text': self.corpus[i],\n",
    "                    'score': float(scores[i])\n",
    "                })\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Абстрактный класс для реранкеров\n",
    "class Reranker(ABC):\n",
    "    @abstractmethod\n",
    "    def rerank(self, query: str, documents: List[Dict[str, Any]], top_k: int = None) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Переранжирует документы на основе запроса.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_reranker_name(self) -> str:\n",
    "        \"\"\"Возвращает имя реранкера.\"\"\"\n",
    "        pass\n",
    "\n",
    "# Реализация без реранкера (проходной реранкер)\n",
    "class NoReranker(Reranker):\n",
    "    def rerank(self, query: str, documents: List[Dict[str, Any]], top_k: int = None) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Возвращает документы без изменения порядка.\"\"\"\n",
    "        if top_k is not None and top_k < len(documents):\n",
    "            return documents[:top_k]\n",
    "        return documents\n",
    "    \n",
    "    def get_reranker_name(self) -> str:\n",
    "        return \"No-Reranker\"\n",
    "    \n",
    "# Реранкер на основе CrossEncoder\n",
    "class CrossEncoderReranker(Reranker):\n",
    "    def __init__(self, model_name: str):\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        # Настройки для токенизатора перед созданием модели\n",
    "        self.max_length = 512  # Стандартный предел для большинства моделей\n",
    "        \n",
    "        # Создаем модель с указанием максимальной длины для токенизатора\n",
    "        self.model = CrossEncoder(\n",
    "            model_name,\n",
    "            max_length=self.max_length,  # Устанавливаем максимальную длину при инициализации\n",
    "            device=None  # Автоматический выбор устройства\n",
    "        )\n",
    "    \n",
    "    def truncate_text(self, text: str, max_chars: int = 1500) -> str:\n",
    "        \"\"\"Обрезает текст до заданного количества символов.\"\"\"\n",
    "        if len(text) <= max_chars:\n",
    "            return text\n",
    "        return text[:max_chars] + \"...\"\n",
    "    \n",
    "    def rerank(self, query: str, documents: List[Dict[str, Any]], top_k: int = None) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Переранжирует документы с использованием CrossEncoder.\"\"\"\n",
    "        if not documents:\n",
    "            return []\n",
    "        \n",
    "        # Подготовка пар (запрос, документ) для оценки, обрезая длинные тексты\n",
    "        pairs = []\n",
    "        for doc in documents:\n",
    "            # Обрезаем длинные документы, чтобы избежать проблем с токенизацией\n",
    "            truncated_text = self.truncate_text(doc['text'])\n",
    "            pairs.append((query, truncated_text))\n",
    "        \n",
    "        # Получение оценок - без указания max_length, т.к. он уже указан при инициализации\n",
    "        scores = self.model.predict(\n",
    "            pairs,\n",
    "            show_progress_bar=False\n",
    "        )\n",
    "        \n",
    "        # Обновление оценок в документах\n",
    "        for i, score in enumerate(scores):\n",
    "            documents[i]['rerank_score'] = float(score)\n",
    "        \n",
    "        # Сортировка по новым оценкам\n",
    "        reranked_docs = sorted(documents, key=lambda x: x['rerank_score'], reverse=True)\n",
    "        \n",
    "        # Ограничение количества результатов, если нужно\n",
    "        if top_k is not None and top_k < len(reranked_docs):\n",
    "            return reranked_docs[:top_k]\n",
    "        return reranked_docs\n",
    "    \n",
    "    def get_reranker_name(self) -> str:\n",
    "        return f\"CrossEncoder-{self.model_name.split('/')[-1]}\"\n",
    "\n",
    "# Реранкер на основе HuggingFace моделей\n",
    "class HuggingFaceReranker(Reranker):\n",
    "    def __init__(self, model_name: str):\n",
    "        self.model_name = model_name\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Определяем максимальную длину для токенизатора\n",
    "        self.max_length = min(\n",
    "            self.tokenizer.model_max_length if hasattr(self.tokenizer, \"model_max_length\") else 512,\n",
    "            512  # Установим верхний предел в 512 токенов\n",
    "        )\n",
    "    \n",
    "    def truncate_text(self, text: str, max_chars: int = 1500) -> str:\n",
    "        \"\"\"Обрезает текст до заданного количества символов.\"\"\"\n",
    "        if len(text) <= max_chars:\n",
    "            return text\n",
    "        return text[:max_chars] + \"...\"\n",
    "    \n",
    "    def rerank(self, query: str, documents: List[Dict[str, Any]], top_k: int = None) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Переранжирует документы с использованием HuggingFace модели.\"\"\"\n",
    "        if not documents:\n",
    "            return []\n",
    "        \n",
    "        reranked_docs = []\n",
    "        \n",
    "        # Обрабатываем документы по одному для экономии памяти\n",
    "        for doc in documents:\n",
    "            # Обрезаем текст, чтобы избежать проблем с длинными документами\n",
    "            truncated_text = self.truncate_text(doc['text'])\n",
    "            \n",
    "            # Токенизация пары запрос-документ\n",
    "            inputs = self.tokenizer(\n",
    "                query, \n",
    "                truncated_text, \n",
    "                padding=True, \n",
    "                truncation=True, \n",
    "                max_length=self.max_length, \n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            # Перемещаем входные данные на GPU, если доступно\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Получаем оценку из модели\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                scores = outputs.logits\n",
    "                \n",
    "                # Обработка в зависимости от формата выходных данных модели\n",
    "                if scores.shape[1] == 1:  # Одна оценка (регрессия)\n",
    "                    score = scores.item()\n",
    "                else:  # Бинарная классификация (релевантный/нерелевантный)\n",
    "                    score = torch.softmax(scores, dim=1)[:, 1].item()\n",
    "            \n",
    "            # Создаем копию документа с обновленной оценкой\n",
    "            doc_copy = doc.copy()\n",
    "            doc_copy['rerank_score'] = score\n",
    "            reranked_docs.append(doc_copy)\n",
    "        \n",
    "        # Сортировка по новым оценкам\n",
    "        reranked_docs = sorted(reranked_docs, key=lambda x: x['rerank_score'], reverse=True)\n",
    "        \n",
    "        # Ограничение количества результатов, если нужно\n",
    "        if top_k is not None and top_k < len(reranked_docs):\n",
    "            return reranked_docs[:top_k]\n",
    "        return reranked_docs\n",
    "    \n",
    "    def get_reranker_name(self) -> str:\n",
    "        return f\"HF-{self.model_name.split('/')[-1]}\"\n",
    "    \n",
    "# Специализированный реранкер для nboost/pt-tinybert-msmarco\n",
    "class TinyBertReranker(Reranker):\n",
    "    def __init__(self, model_name: str = 'nboost/pt-tinybert-msmarco'):\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        # Загружаем модель и токенизатор напрямую из HuggingFace\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        self.max_length = 512\n",
    "    \n",
    "    def truncate_text(self, text: str, max_chars: int = 1500) -> str:\n",
    "        \"\"\"Обрезает текст до заданного количества символов.\"\"\"\n",
    "        if len(text) <= max_chars:\n",
    "            return text\n",
    "        return text[:max_chars] + \"...\"\n",
    "    \n",
    "    def rerank(self, query: str, documents: List[Dict[str, Any]], top_k: int = None) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Переранжирует документы с использованием TinyBERT напрямую.\"\"\"\n",
    "        if not documents:\n",
    "            return []\n",
    "        \n",
    "        reranked_docs = []\n",
    "        \n",
    "        for doc in documents:\n",
    "            try:\n",
    "                # Обрезаем текст\n",
    "                truncated_text = self.truncate_text(doc['text'])\n",
    "                \n",
    "                # Токенизация\n",
    "                inputs = self.tokenizer(\n",
    "                    query,\n",
    "                    truncated_text,\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=self.max_length,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                \n",
    "                # Перемещаем на устройство\n",
    "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "                \n",
    "                # Получаем оценку\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model(**inputs)\n",
    "                    logits = outputs.logits\n",
    "                    \n",
    "                    # TinyBERT для MS MARCO обычно возвращает один скор\n",
    "                    score = float(logits[0][0].cpu().numpy())\n",
    "                \n",
    "                # Создаем копию с оценкой\n",
    "                doc_copy = doc.copy()\n",
    "                doc_copy['rerank_score'] = score\n",
    "                reranked_docs.append(doc_copy)\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Ошибка при ранжировании с TinyBERT: {e}\")\n",
    "                # Добавляем с нулевой оценкой\n",
    "                doc_copy = doc.copy()\n",
    "                doc_copy['rerank_score'] = 0.0\n",
    "                reranked_docs.append(doc_copy)\n",
    "        \n",
    "        # Сортировка\n",
    "        reranked_docs = sorted(reranked_docs, key=lambda x: x['rerank_score'], reverse=True)\n",
    "        \n",
    "        # Ограничение количества\n",
    "        if top_k is not None and top_k < len(reranked_docs):\n",
    "            return reranked_docs[:top_k]\n",
    "        return reranked_docs\n",
    "    \n",
    "    def get_reranker_name(self) -> str:\n",
    "        return \"TinyBERT-MSMARCO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Модифицированный класс QdrantManager для поддержки разных моделей\n",
    "class QdrantManager:\n",
    "    \"\"\"Класс для работы с базой Qdrant.\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_model: EmbeddingModel = None):\n",
    "        self.client = QdrantClient(\":memory:\")\n",
    "        self.collection_name = \"documentation\"\n",
    "        self.embedding_model = embedding_model\n",
    "        # Для сохранения оригинальных текстов (нужно для BM25)\n",
    "        self.corpus = []\n",
    "        self.id_to_idx = {}  # Отображение id -> индекс в corpus\n",
    "    \n",
    "    def set_embedding_model(self, embedding_model: EmbeddingModel) -> None:\n",
    "        \"\"\"Устанавливает модель для эмбеддинга.\"\"\"\n",
    "        self.embedding_model = embedding_model\n",
    "    \n",
    "    def initialize_collection(self, vector_size: int = None):\n",
    "        \"\"\"Инициализирует коллекцию в Qdrant.\"\"\"\n",
    "        if vector_size is None and self.embedding_model is not None:\n",
    "            vector_size = self.embedding_model.get_vector_size()\n",
    "        \n",
    "        # Удаление существующей коллекции, если она есть\n",
    "        try:\n",
    "            self.client.delete_collection(collection_name=self.collection_name)\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        # Создание новой коллекции\n",
    "        self.client.create_collection(\n",
    "            collection_name=self.collection_name,\n",
    "            vectors_config=models.VectorParams(\n",
    "                size=vector_size,\n",
    "                distance=models.Distance.COSINE\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Очистка корпуса при инициализации новой коллекции\n",
    "        self.corpus = []\n",
    "        self.id_to_idx = {}\n",
    "    \n",
    "    def upsert_batch(self, id_offset: int, vectors: List[List[float]], \n",
    "                    payloads: List[Dict[str, Any]]) -> None:\n",
    "        \"\"\"Добавляет партию данных в Qdrant.\"\"\"\n",
    "        # Сохраняем тексты в корпусе (для BM25)\n",
    "        for idx, payload in enumerate(payloads):\n",
    "            corpus_idx = len(self.corpus)\n",
    "            self.corpus.append(payload['text'])\n",
    "            self.id_to_idx[idx + id_offset] = corpus_idx\n",
    "        \n",
    "        # Добавляем в Qdrant\n",
    "        points = [\n",
    "            models.PointStruct(\n",
    "                id=idx + id_offset,\n",
    "                vector=vector,\n",
    "                payload=payload\n",
    "            )\n",
    "            for idx, (vector, payload) in enumerate(zip(vectors, payloads))\n",
    "        ]\n",
    "        \n",
    "        self.client.upsert(\n",
    "            collection_name=self.collection_name,\n",
    "            points=points\n",
    "        )\n",
    "    \n",
    "    def search(self, query: str, limit: int = 6) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Выполняет поиск похожих документов.\"\"\"\n",
    "        # Для BM25 используем прямой поиск\n",
    "        if isinstance(self.embedding_model, BM25Model):\n",
    "            return self.embedding_model.search(query, limit=limit)\n",
    "        \n",
    "        # Для других моделей используем Qdrant\n",
    "        query_vector = self.embedding_model.compute_single_embedding(query)\n",
    "        search_result = self.client.search(\n",
    "            collection_name=self.collection_name,\n",
    "            query_vector=query_vector,\n",
    "            limit=limit\n",
    "        )\n",
    "        \n",
    "        return [\n",
    "            {\n",
    "                'text': hit.payload['text'],\n",
    "                'id': hit.payload['id'],\n",
    "                'score': hit.score\n",
    "            }\n",
    "            for hit in search_result\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsCalculator:\n",
    "    \"\"\"Класс для расчета метрик оценки качества поиска.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_recall_at_k(relevant_ids: List[int], retrieved_ids: List[int], k: int) -> float:\n",
    "        \"\"\"Рассчитывает Recall@k для одного запроса.\"\"\"\n",
    "        if not relevant_ids:\n",
    "            return 0.0\n",
    "        \n",
    "        relevant_retrieved = set(relevant_ids).intersection(set(retrieved_ids[:k]))\n",
    "        return len(relevant_retrieved) / len(relevant_ids)\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_precision_at_k(relevant_ids: List[int], retrieved_ids: List[int], k: int) -> float:\n",
    "        \"\"\"Рассчитывает Precision@k для одного запроса.\"\"\"\n",
    "        if k == 0 or not retrieved_ids:\n",
    "            return 0.0\n",
    "        \n",
    "        relevant_retrieved = set(relevant_ids).intersection(set(retrieved_ids[:k]))\n",
    "        return len(relevant_retrieved) / min(k, len(retrieved_ids))\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_mrr_at_k(relevant_ids: List[int], retrieved_ids: List[int], k: int) -> float:\n",
    "        \"\"\"Рассчитывает MRR@k (Mean Reciprocal Rank) для одного запроса.\"\"\"\n",
    "        if not relevant_ids or not retrieved_ids:\n",
    "            return 0.0\n",
    "        \n",
    "        for i, doc_id in enumerate(retrieved_ids[:k]):\n",
    "            if doc_id in relevant_ids:\n",
    "                return 1.0 / (i + 1)\n",
    "        return 0.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_ndcg_at_k(relevant_ids: List[int], retrieved_ids: List[int], k: int) -> float:\n",
    "        \"\"\"Рассчитывает nDCG@k для одного запроса вручную.\"\"\"\n",
    "        if not relevant_ids or not retrieved_ids or k <= 0:\n",
    "            return 0.0\n",
    "        \n",
    "        # Обрезаем список до k элементов\n",
    "        retrieved_ids_k = retrieved_ids[:k]\n",
    "        \n",
    "        # Создаем вектор релевантности (1 для релевантных документов, 0 для нерелевантных)\n",
    "        relevance = [1.0 if doc_id in relevant_ids else 0.0 for doc_id in retrieved_ids_k]\n",
    "        \n",
    "        # Если нет релевантных документов среди извлеченных, возвращаем 0\n",
    "        if sum(relevance) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        # Вычисляем DCG (Discounted Cumulative Gain)\n",
    "        dcg = 0.0\n",
    "        for i, rel in enumerate(relevance):\n",
    "            # Используем формулу DCG = rel_1 + rel_2/log2(2+1) + rel_3/log2(3+1) + ...\n",
    "            if rel > 0:\n",
    "                dcg += rel / np.log2(i + 2)  # +2 потому что индексация с 0, и log2(1)=0\n",
    "        \n",
    "        # Вычисляем идеальный DCG (IDCG)\n",
    "        # В идеальном случае релевантные документы находятся в начале списка\n",
    "        ideal_relevance = sorted(relevance, reverse=True)\n",
    "        idcg = 0.0\n",
    "        for i, rel in enumerate(ideal_relevance):\n",
    "            if rel > 0:\n",
    "                idcg += rel / np.log2(i + 2)\n",
    "        \n",
    "        # nDCG = DCG / IDCG\n",
    "        return dcg / idcg if idcg > 0 else 0.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_average_metrics(metrics_list: List[Dict[str, float]]) -> Dict[str, float]:\n",
    "        \"\"\"Вычисляет средние значения метрик по всем запросам.\"\"\"\n",
    "        if not metrics_list:\n",
    "            return {}\n",
    "        \n",
    "        # Инициализируем результирующий словарь\n",
    "        result = {}\n",
    "        \n",
    "        # Собираем все ключи метрик\n",
    "        all_keys = set()\n",
    "        for metrics in metrics_list:\n",
    "            all_keys.update(metrics.keys())\n",
    "        \n",
    "        # Вычисляем среднее значение для каждой метрики\n",
    "        for key in all_keys:\n",
    "            values = [metrics.get(key, 0.0) for metrics in metrics_list]\n",
    "            result[key] = sum(values) / len(metrics_list)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentationQA:\n",
    "    \"\"\"Главный класс, объединяющий все компоненты системы.\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_model: Optional[EmbeddingModel] = None, reranker: Optional[Reranker] = None):\n",
    "        # Если модель не передана, используем SentenceTransformer по умолчанию\n",
    "        self.embedding_model = embedding_model or SentenceTransformerModel()\n",
    "        # Если реранкер не передан, используем NoReranker\n",
    "        self.reranker = reranker or NoReranker()\n",
    "        self.qdrant_manager = QdrantManager(self.embedding_model)\n",
    "        self.metrics_calculator = MetricsCalculator()\n",
    "        self.is_initialized = False\n",
    "        self.df = None\n",
    "        self.section_id_map = {}  # Для хранения мапинга section_content -> id\n",
    "    \n",
    "    def set_embedding_model(self, embedding_model: EmbeddingModel) -> None:\n",
    "        \"\"\"Устанавливает модель для эмбеддинга и сбрасывает инициализацию.\"\"\"\n",
    "        self.embedding_model = embedding_model\n",
    "        self.qdrant_manager.set_embedding_model(embedding_model)\n",
    "        self.is_initialized = False\n",
    "    \n",
    "    def set_reranker(self, reranker: Reranker) -> None:\n",
    "        \"\"\"Устанавливает реранкер.\"\"\"\n",
    "        self.reranker = reranker\n",
    "    \n",
    "    def load_data(self, file_path: str = 'qdrant_documentation_dataset.csv'):\n",
    "        \"\"\"Загружает данные из CSV файла.\"\"\"\n",
    "        self.df = pd.read_csv(file_path)\n",
    "        print(f\"Загружено {len(self.df)} записей из датасета.\")\n",
    "    \n",
    "    def initialize_database(self):\n",
    "        \"\"\"Инициализирует базу данных, добавляя чанки из датасета в Qdrant.\"\"\"\n",
    "        if self.is_initialized:\n",
    "            return\n",
    "        \n",
    "        if self.df is None:\n",
    "            raise ValueError(\"Данные не загружены. Вызовите метод load_data() перед инициализацией базы.\")\n",
    "        \n",
    "        # Получаем уникальные чанки контента\n",
    "        sections = self.df['section_content'].unique()\n",
    "        print(f\"Найдено {len(sections)} уникальных чанков для индексации.\")\n",
    "        \n",
    "        # Создаем мапинг section_content -> id\n",
    "        for idx, section in enumerate(sections):\n",
    "            self.section_id_map[section] = idx\n",
    "        \n",
    "        # Инициализация коллекции Qdrant\n",
    "        vector_size = self.embedding_model.get_vector_size()\n",
    "        self.qdrant_manager.initialize_collection(vector_size=vector_size)\n",
    "        \n",
    "        # Специальная обработка для BM25\n",
    "        if isinstance(self.embedding_model, BM25Model):\n",
    "            self.embedding_model.initialize_corpus(sections)\n",
    "        \n",
    "        # Вычисление эмбеддингов и добавление в Qdrant\n",
    "        batch_size = 100\n",
    "        for i in range(0, len(sections), batch_size):\n",
    "            batch = sections[i:i + batch_size]\n",
    "            \n",
    "            # Вычисление эмбеддингов\n",
    "            print(f\"Вычисление эмбеддингов для батча {i//batch_size + 1}/{(len(sections) - 1)//batch_size + 1}\")\n",
    "            embeddings = self.embedding_model.compute_embeddings(batch)\n",
    "            \n",
    "            # Подготовка payload\n",
    "            payloads = [\n",
    "                {\n",
    "                    'text': section,\n",
    "                    'id': self.section_id_map[section]\n",
    "                }\n",
    "                for section in batch\n",
    "            ]\n",
    "            \n",
    "            # Добавление в Qdrant\n",
    "            self.qdrant_manager.upsert_batch(i, embeddings, payloads)\n",
    "        \n",
    "        self.is_initialized = True\n",
    "        print(f\"База данных успешно инициализирована с моделью {self.embedding_model.get_model_name()}.\")\n",
    "    \n",
    "    def search_similar_sections(self, query: str, top_k: int = 6, first_stage_k: int = None) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Ищет чанки, похожие на запрос, с применением реранкинга.\"\"\"\n",
    "        # Определяем, сколько документов получить на первом этапе\n",
    "        if first_stage_k is None:\n",
    "            # По умолчанию берем больше документов для реранкинга, если используется реранкер\n",
    "            first_stage_k = top_k * 3 if not isinstance(self.reranker, NoReranker) else top_k\n",
    "        \n",
    "        # Поиск через менеджер Qdrant (первый этап)\n",
    "        first_stage_results = self.qdrant_manager.search(query, limit=first_stage_k)\n",
    "        \n",
    "        # Применяем реранкинг (второй этап)\n",
    "        reranked_results = self.reranker.rerank(query, first_stage_results, top_k=top_k)\n",
    "        \n",
    "        return reranked_results\n",
    "    \n",
    "    def evaluate_model(self, k_values: List[int] = [1, 4, 6], first_stage_k: int = None) -> Dict[str, float]:\n",
    "        \"\"\"Оценивает модель по различным метрикам.\"\"\"\n",
    "        if not self.is_initialized:\n",
    "            self.initialize_database()\n",
    "        \n",
    "        all_metrics = []\n",
    "        model_name = self.embedding_model.get_model_name()\n",
    "        reranker_name = self.reranker.get_reranker_name()\n",
    "        \n",
    "        print(f\"Оценка модели: {model_name} с реранкером: {reranker_name}\")\n",
    "        \n",
    "        # Перебираем все вопросы в датасете\n",
    "        for idx, row in tqdm(self.df.iterrows(), total=len(self.df), desc=\"Оценка модели\"):\n",
    "            question = row['question']\n",
    "            relevant_section = row['section_content']\n",
    "            relevant_id = self.section_id_map[relevant_section]\n",
    "            \n",
    "            # Получаем результаты поиска с реранкингом\n",
    "            search_results = self.search_similar_sections(\n",
    "                question, \n",
    "                top_k=max(k_values), \n",
    "                first_stage_k=first_stage_k\n",
    "            )\n",
    "            retrieved_ids = [result['id'] for result in search_results]\n",
    "            \n",
    "            # Рассчитываем метрики для текущего запроса\n",
    "            query_metrics = {}\n",
    "            for k in k_values:\n",
    "                query_metrics[f'Recall@{k}'] = self.metrics_calculator.calculate_recall_at_k([relevant_id], retrieved_ids, k)\n",
    "                query_metrics[f'Precision@{k}'] = self.metrics_calculator.calculate_precision_at_k([relevant_id], retrieved_ids, k)\n",
    "                query_metrics[f'MRR@{k}'] = self.metrics_calculator.calculate_mrr_at_k([relevant_id], retrieved_ids, k)\n",
    "                query_metrics[f'nDCG@{k}'] = self.metrics_calculator.calculate_ndcg_at_k([relevant_id], retrieved_ids, k)\n",
    "            \n",
    "            all_metrics.append(query_metrics)\n",
    "        \n",
    "        # Вычисляем средние метрики\n",
    "        average_metrics = self.metrics_calculator.compute_average_metrics(all_metrics)\n",
    "        \n",
    "        # Добавляем информацию о модели и реранкере\n",
    "        average_metrics['model_name'] = model_name\n",
    "        average_metrics['reranker_name'] = reranker_name\n",
    "        \n",
    "        return average_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для тестирования нескольких моделей и сравнения их результатов\n",
    "def compare_models(data_path: str = 'qdrant_documentation_dataset.csv', k_values: List[int] = [1, 4, 6]):\n",
    "    \"\"\"Сравнивает различные модели и выводит их метрики.\"\"\"\n",
    "    # Создаем объект DocumentationQA\n",
    "    qa_system = DocumentationQA()\n",
    "    \n",
    "    # Загружаем данные\n",
    "    qa_system.load_data(data_path)\n",
    "    \n",
    "    # Список моделей для тестирования\n",
    "    models = [\n",
    "        SentenceTransformerModel('BAAI/bge-large-en-v1.5'),\n",
    "        SentenceTransformerModel('intfloat/multilingual-e5-large'),\n",
    "        BM25Model()\n",
    "    ]\n",
    "    \n",
    "    # Оценка каждой модели\n",
    "    results = []\n",
    "    for model in models:\n",
    "        print(f\"\\n===== Тестирование модели: {model.get_model_name()} =====\")\n",
    "        \n",
    "        # Устанавливаем модель\n",
    "        qa_system.set_embedding_model(model)\n",
    "        \n",
    "        # Инициализируем базу данных с новой моделью\n",
    "        qa_system.initialize_database()\n",
    "        \n",
    "        # Оцениваем модель\n",
    "        metrics = qa_system.evaluate_model(k_values=k_values)\n",
    "        results.append(metrics)\n",
    "        \n",
    "        # Выводим результаты для текущей модели\n",
    "        print(\"\\nМетрики:\")\n",
    "        for metric_name, value in sorted(metrics.items()):\n",
    "            if metric_name != 'model_name':\n",
    "                print(f\"{metric_name}: {value:.4f}\")\n",
    "    \n",
    "    # Сравнительная таблица метрик\n",
    "    print(\"\\n===== Сравнение моделей =====\")\n",
    "    \n",
    "    # Получаем все метрики\n",
    "    all_metric_names = set()\n",
    "    for result in results:\n",
    "        all_metric_names.update([k for k in result.keys() if k != 'model_name'])\n",
    "    \n",
    "    # Форматируем таблицу\n",
    "    metric_names_sorted = sorted(all_metric_names)\n",
    "    model_names = [result['model_name'] for result in results]\n",
    "    \n",
    "    # Верхний заголовок таблицы\n",
    "    header = f\"{'Метрика':<15} | \" + \" | \".join(f\"{name:<25}\" for name in model_names)\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "    \n",
    "    # Строки таблицы\n",
    "    for metric in metric_names_sorted:\n",
    "        row = f\"{metric:<15} | \"\n",
    "        for result in results:\n",
    "            value = result.get(metric, 0.0)\n",
    "            row += f\"{value:.4f}{' ' * (25 - len(f'{value:.4f}'))}\"\n",
    "            row += \" | \"\n",
    "        print(row[:-3])  # Убираем лишний разделитель в конце\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружено 328 записей из датасета.\n",
      "Найдено 121 уникальных чанков для индексации.\n",
      "Вычисление эмбеддингов для батча 1/2\n",
      "Вычисление эмбеддингов для батча 2/2\n",
      "База данных успешно инициализирована с моделью SentenceTransformer-bge-base-en-v1.5.\n",
      "\n",
      "===== Модель: SentenceTransformer-bge-base-en-v1.5 + Реранкер: None =====\n",
      "Оценка модели: SentenceTransformer-bge-base-en-v1.5 с реранкером: No-Reranker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Оценка модели:   0%|          | 0/328 [00:00<?, ?it/s]C:\\Users\\sekho\\AppData\\Local\\Temp\\ipykernel_25372\\1846738923.py:73: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_result = self.client.search(\n",
      "Оценка модели: 100%|██████████| 328/328 [00:13<00:00, 24.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Метрики:\n",
      "\n",
      "Recall@k:\n",
      "  Recall@1: 0.6433\n",
      "  Recall@4: 0.8811\n",
      "  Recall@6: 0.9055\n",
      "\n",
      "Precision@k:\n",
      "  Precision@1: 0.6433\n",
      "  Precision@4: 0.2203\n",
      "  Precision@6: 0.1509\n",
      "\n",
      "MRR@k:\n",
      "  MRR@1: 0.6433\n",
      "  MRR@4: 0.7409\n",
      "  MRR@6: 0.7455\n",
      "\n",
      "nDCG@k:\n",
      "  nDCG@1: 0.6433\n",
      "  nDCG@4: 0.7764\n",
      "  nDCG@6: 0.7856\n",
      "\n",
      "==================================================\n",
      "\n",
      "===== Модель: SentenceTransformer-bge-base-en-v1.5 + Реранкер: cross-encoder/ms-marco-MiniLM-L-12-v2 =====\n",
      "Оценка модели: SentenceTransformer-bge-base-en-v1.5 с реранкером: CrossEncoder-ms-marco-MiniLM-L-12-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Оценка модели: 100%|██████████| 328/328 [11:16<00:00,  2.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Метрики:\n",
      "\n",
      "Recall@k:\n",
      "  Recall@1: 0.7104\n",
      "  Recall@4: 0.8780\n",
      "  Recall@6: 0.9024\n",
      "\n",
      "Precision@k:\n",
      "  Precision@1: 0.7104\n",
      "  Precision@4: 0.2195\n",
      "  Precision@6: 0.1504\n",
      "\n",
      "MRR@k:\n",
      "  MRR@1: 0.7104\n",
      "  MRR@4: 0.7790\n",
      "  MRR@6: 0.7836\n",
      "\n",
      "nDCG@k:\n",
      "  nDCG@1: 0.7104\n",
      "  nDCG@4: 0.8041\n",
      "  nDCG@6: 0.8133\n",
      "\n",
      "==================================================\n",
      "\n",
      "===== Модель: SentenceTransformer-bge-base-en-v1.5 + Реранкер: BAAI/bge-reranker-base =====\n",
      "Оценка модели: SentenceTransformer-bge-base-en-v1.5 с реранкером: CrossEncoder-bge-reranker-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Оценка модели: 100%|██████████| 328/328 [37:53<00:00,  6.93s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Метрики:\n",
      "\n",
      "Recall@k:\n",
      "  Recall@1: 0.7043\n",
      "  Recall@4: 0.8720\n",
      "  Recall@6: 0.8872\n",
      "\n",
      "Precision@k:\n",
      "  Precision@1: 0.7043\n",
      "  Precision@4: 0.2180\n",
      "  Precision@6: 0.1479\n",
      "\n",
      "MRR@k:\n",
      "  MRR@1: 0.7043\n",
      "  MRR@4: 0.7713\n",
      "  MRR@6: 0.7743\n",
      "\n",
      "nDCG@k:\n",
      "  nDCG@1: 0.7043\n",
      "  nDCG@4: 0.7968\n",
      "  nDCG@6: 0.8026\n",
      "\n",
      "==================================================\n",
      "\n",
      "===== Модель: SentenceTransformer-bge-base-en-v1.5 + Реранкер: TinyBERT-MSMARCO =====\n",
      "Оценка модели: SentenceTransformer-bge-base-en-v1.5 с реранкером: TinyBERT-MSMARCO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Оценка модели: 100%|██████████| 328/328 [02:51<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Метрики:\n",
      "\n",
      "Recall@k:\n",
      "  Recall@1: 0.0061\n",
      "  Recall@4: 0.0183\n",
      "  Recall@6: 0.0305\n",
      "\n",
      "Precision@k:\n",
      "  Precision@1: 0.0061\n",
      "  Precision@4: 0.0046\n",
      "  Precision@6: 0.0051\n",
      "\n",
      "MRR@k:\n",
      "  MRR@1: 0.0061\n",
      "  MRR@4: 0.0094\n",
      "  MRR@6: 0.0117\n",
      "\n",
      "nDCG@k:\n",
      "  nDCG@1: 0.0061\n",
      "  nDCG@4: 0.0116\n",
      "  nDCG@6: 0.0162\n",
      "\n",
      "==================================================\n",
      "\n",
      "===== Модель: SentenceTransformer-bge-base-en-v1.5 + Реранкер: sentence-transformers/msmarco-distilbert-base-tas-b =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/msmarco-distilbert-base-tas-b and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оценка модели: SentenceTransformer-bge-base-en-v1.5 с реранкером: CrossEncoder-msmarco-distilbert-base-tas-b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Оценка модели: 100%|██████████| 328/328 [14:37<00:00,  2.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Метрики:\n",
      "\n",
      "Recall@k:\n",
      "  Recall@1: 0.0335\n",
      "  Recall@4: 0.1463\n",
      "  Recall@6: 0.2378\n",
      "\n",
      "Precision@k:\n",
      "  Precision@1: 0.0335\n",
      "  Precision@4: 0.0366\n",
      "  Precision@6: 0.0396\n",
      "\n",
      "MRR@k:\n",
      "  MRR@1: 0.0335\n",
      "  MRR@4: 0.0757\n",
      "  MRR@6: 0.0923\n",
      "\n",
      "nDCG@k:\n",
      "  nDCG@1: 0.0335\n",
      "  nDCG@4: 0.0934\n",
      "  nDCG@6: 0.1272\n",
      "\n",
      "==================================================\n",
      "Найдено 121 уникальных чанков для индексации.\n",
      "Вычисление эмбеддингов для батча 1/2\n",
      "Вычисление эмбеддингов для батча 2/2\n",
      "База данных успешно инициализирована с моделью SentenceTransformer-multilingual-e5-base.\n",
      "\n",
      "===== Модель: SentenceTransformer-multilingual-e5-base + Реранкер: None =====\n",
      "Оценка модели: SentenceTransformer-multilingual-e5-base с реранкером: No-Reranker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Оценка модели: 100%|██████████| 328/328 [00:11<00:00, 29.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Метрики:\n",
      "\n",
      "Recall@k:\n",
      "  Recall@1: 0.5610\n",
      "  Recall@4: 0.7866\n",
      "  Recall@6: 0.8293\n",
      "\n",
      "Precision@k:\n",
      "  Precision@1: 0.5610\n",
      "  Precision@4: 0.1966\n",
      "  Precision@6: 0.1382\n",
      "\n",
      "MRR@k:\n",
      "  MRR@1: 0.5610\n",
      "  MRR@4: 0.6502\n",
      "  MRR@6: 0.6583\n",
      "\n",
      "nDCG@k:\n",
      "  nDCG@1: 0.5610\n",
      "  nDCG@4: 0.6846\n",
      "  nDCG@6: 0.7007\n",
      "\n",
      "==================================================\n",
      "\n",
      "===== Модель: SentenceTransformer-multilingual-e5-base + Реранкер: cross-encoder/ms-marco-MiniLM-L-12-v2 =====\n",
      "Оценка модели: SentenceTransformer-multilingual-e5-base с реранкером: CrossEncoder-ms-marco-MiniLM-L-12-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Оценка модели: 100%|██████████| 328/328 [08:53<00:00,  1.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Метрики:\n",
      "\n",
      "Recall@k:\n",
      "  Recall@1: 0.7043\n",
      "  Recall@4: 0.8689\n",
      "  Recall@6: 0.8902\n",
      "\n",
      "Precision@k:\n",
      "  Precision@1: 0.7043\n",
      "  Precision@4: 0.2172\n",
      "  Precision@6: 0.1484\n",
      "\n",
      "MRR@k:\n",
      "  MRR@1: 0.7043\n",
      "  MRR@4: 0.7724\n",
      "  MRR@6: 0.7764\n",
      "\n",
      "nDCG@k:\n",
      "  nDCG@1: 0.7043\n",
      "  nDCG@4: 0.7969\n",
      "  nDCG@6: 0.8049\n",
      "\n",
      "==================================================\n",
      "\n",
      "===== Модель: SentenceTransformer-multilingual-e5-base + Реранкер: BAAI/bge-reranker-base =====\n",
      "Оценка модели: SentenceTransformer-multilingual-e5-base с реранкером: CrossEncoder-bge-reranker-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Оценка модели: 100%|██████████| 328/328 [25:19<00:00,  4.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Метрики:\n",
      "\n",
      "Recall@k:\n",
      "  Recall@1: 0.7104\n",
      "  Recall@4: 0.8659\n",
      "  Recall@6: 0.8902\n",
      "\n",
      "Precision@k:\n",
      "  Precision@1: 0.7104\n",
      "  Precision@4: 0.2165\n",
      "  Precision@6: 0.1484\n",
      "\n",
      "MRR@k:\n",
      "  MRR@1: 0.7104\n",
      "  MRR@4: 0.7724\n",
      "  MRR@6: 0.7767\n",
      "\n",
      "nDCG@k:\n",
      "  nDCG@1: 0.7104\n",
      "  nDCG@4: 0.7960\n",
      "  nDCG@6: 0.8049\n",
      "\n",
      "==================================================\n",
      "\n",
      "===== Модель: SentenceTransformer-multilingual-e5-base + Реранкер: TinyBERT-MSMARCO =====\n",
      "Оценка модели: SentenceTransformer-multilingual-e5-base с реранкером: TinyBERT-MSMARCO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Оценка модели: 100%|██████████| 328/328 [02:14<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Метрики:\n",
      "\n",
      "Recall@k:\n",
      "  Recall@1: 0.0000\n",
      "  Recall@4: 0.0213\n",
      "  Recall@6: 0.0305\n",
      "\n",
      "Precision@k:\n",
      "  Precision@1: 0.0000\n",
      "  Precision@4: 0.0053\n",
      "  Precision@6: 0.0051\n",
      "\n",
      "MRR@k:\n",
      "  MRR@1: 0.0000\n",
      "  MRR@4: 0.0053\n",
      "  MRR@6: 0.0070\n",
      "\n",
      "nDCG@k:\n",
      "  nDCG@1: 0.0000\n",
      "  nDCG@4: 0.0092\n",
      "  nDCG@6: 0.0125\n",
      "\n",
      "==================================================\n",
      "\n",
      "===== Модель: SentenceTransformer-multilingual-e5-base + Реранкер: sentence-transformers/msmarco-distilbert-base-tas-b =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/msmarco-distilbert-base-tas-b and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оценка модели: SentenceTransformer-multilingual-e5-base с реранкером: CrossEncoder-msmarco-distilbert-base-tas-b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Оценка модели: 100%|██████████| 328/328 [11:18<00:00,  2.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Метрики:\n",
      "\n",
      "Recall@k:\n",
      "  Recall@1: 0.0366\n",
      "  Recall@4: 0.1890\n",
      "  Recall@6: 0.2866\n",
      "\n",
      "Precision@k:\n",
      "  Precision@1: 0.0366\n",
      "  Precision@4: 0.0473\n",
      "  Precision@6: 0.0478\n",
      "\n",
      "MRR@k:\n",
      "  MRR@1: 0.0366\n",
      "  MRR@4: 0.0899\n",
      "  MRR@6: 0.1082\n",
      "\n",
      "nDCG@k:\n",
      "  nDCG@1: 0.0366\n",
      "  nDCG@4: 0.1146\n",
      "  nDCG@6: 0.1512\n",
      "\n",
      "==================================================\n",
      "Найдено 121 уникальных чанков для индексации.\n",
      "Вычисление эмбеддингов для батча 1/2\n",
      "Вычисление эмбеддингов для батча 2/2\n",
      "База данных успешно инициализирована с моделью BM25.\n",
      "\n",
      "===== Модель: BM25 + Реранкер: None =====\n",
      "Оценка модели: BM25 с реранкером: No-Reranker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Оценка модели: 100%|██████████| 328/328 [00:00<00:00, 3247.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Метрики:\n",
      "\n",
      "Recall@k:\n",
      "  Recall@1: 0.5823\n",
      "  Recall@4: 0.8262\n",
      "  Recall@6: 0.8628\n",
      "\n",
      "Precision@k:\n",
      "  Precision@1: 0.5823\n",
      "  Precision@4: 0.2066\n",
      "  Precision@6: 0.1438\n",
      "\n",
      "MRR@k:\n",
      "  MRR@1: 0.5823\n",
      "  MRR@4: 0.6855\n",
      "  MRR@6: 0.6923\n",
      "\n",
      "nDCG@k:\n",
      "  nDCG@1: 0.5823\n",
      "  nDCG@4: 0.7213\n",
      "  nDCG@6: 0.7350\n",
      "\n",
      "==================================================\n",
      "\n",
      "===== Модель: BM25 + Реранкер: cross-encoder/ms-marco-MiniLM-L-12-v2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оценка модели: BM25 с реранкером: CrossEncoder-ms-marco-MiniLM-L-12-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Оценка модели: 100%|██████████| 328/328 [07:09<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Метрики:\n",
      "\n",
      "Recall@k:\n",
      "  Recall@1: 0.6829\n",
      "  Recall@4: 0.8506\n",
      "  Recall@6: 0.8750\n",
      "\n",
      "Precision@k:\n",
      "  Precision@1: 0.6829\n",
      "  Precision@4: 0.2127\n",
      "  Precision@6: 0.1458\n",
      "\n",
      "MRR@k:\n",
      "  MRR@1: 0.6829\n",
      "  MRR@4: 0.7523\n",
      "  MRR@6: 0.7570\n",
      "\n",
      "nDCG@k:\n",
      "  nDCG@1: 0.6829\n",
      "  nDCG@4: 0.7772\n",
      "  nDCG@6: 0.7865\n",
      "\n",
      "==================================================\n",
      "\n",
      "===== Модель: BM25 + Реранкер: BAAI/bge-reranker-base =====\n",
      "Оценка модели: BM25 с реранкером: CrossEncoder-bge-reranker-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Оценка модели: 100%|██████████| 328/328 [24:04<00:00,  4.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Метрики:\n",
      "\n",
      "Recall@k:\n",
      "  Recall@1: 0.7073\n",
      "  Recall@4: 0.8506\n",
      "  Recall@6: 0.8872\n",
      "\n",
      "Precision@k:\n",
      "  Precision@1: 0.7073\n",
      "  Precision@4: 0.2127\n",
      "  Precision@6: 0.1479\n",
      "\n",
      "MRR@k:\n",
      "  MRR@1: 0.7073\n",
      "  MRR@4: 0.7645\n",
      "  MRR@6: 0.7714\n",
      "\n",
      "nDCG@k:\n",
      "  nDCG@1: 0.7073\n",
      "  nDCG@4: 0.7862\n",
      "  nDCG@6: 0.8000\n",
      "\n",
      "==================================================\n",
      "\n",
      "===== Модель: BM25 + Реранкер: TinyBERT-MSMARCO =====\n",
      "Оценка модели: BM25 с реранкером: TinyBERT-MSMARCO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Оценка модели: 100%|██████████| 328/328 [02:09<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Метрики:\n",
      "\n",
      "Recall@k:\n",
      "  Recall@1: 0.0030\n",
      "  Recall@4: 0.0152\n",
      "  Recall@6: 0.0274\n",
      "\n",
      "Precision@k:\n",
      "  Precision@1: 0.0030\n",
      "  Precision@4: 0.0038\n",
      "  Precision@6: 0.0046\n",
      "\n",
      "MRR@k:\n",
      "  MRR@1: 0.0030\n",
      "  MRR@4: 0.0071\n",
      "  MRR@6: 0.0093\n",
      "\n",
      "nDCG@k:\n",
      "  nDCG@1: 0.0030\n",
      "  nDCG@4: 0.0091\n",
      "  nDCG@6: 0.0137\n",
      "\n",
      "==================================================\n",
      "\n",
      "===== Модель: BM25 + Реранкер: sentence-transformers/msmarco-distilbert-base-tas-b =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/msmarco-distilbert-base-tas-b and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оценка модели: BM25 с реранкером: CrossEncoder-msmarco-distilbert-base-tas-b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Оценка модели: 100%|██████████| 328/328 [11:20<00:00,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Метрики:\n",
      "\n",
      "Recall@k:\n",
      "  Recall@1: 0.0396\n",
      "  Recall@4: 0.1433\n",
      "  Recall@6: 0.2317\n",
      "\n",
      "Precision@k:\n",
      "  Precision@1: 0.0396\n",
      "  Precision@4: 0.0358\n",
      "  Precision@6: 0.0386\n",
      "\n",
      "MRR@k:\n",
      "  MRR@1: 0.0396\n",
      "  MRR@4: 0.0770\n",
      "  MRR@6: 0.0932\n",
      "\n",
      "nDCG@k:\n",
      "  nDCG@1: 0.0396\n",
      "  nDCG@4: 0.0935\n",
      "  nDCG@6: 0.1264\n",
      "\n",
      "==================================================\n",
      "\n",
      "===== Сводная таблица результатов =====\n",
      "Embedding Model                           Reranker                                    Recall@1  Precision@1  MRR@1   nDCG@1  Recall@4  Precision@4  MRR@4   nDCG@4  Recall@6  Precision@6  MRR@6   nDCG@6  \n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "SentenceTransformer-bge-base-en-v1.5      No-Reranker                                 0.6433    0.6433       0.6433  0.6433  0.8811    0.2203       0.7409  0.7764  0.9055    0.1509       0.7455  0.7856  \n",
      "SentenceTransformer-bge-base-en-v1.5      CrossEncoder-ms-marco-MiniLM-L-12-v2        0.7104    0.7104       0.7104  0.7104  0.8780    0.2195       0.7790  0.8041  0.9024    0.1504       0.7836  0.8133  \n",
      "SentenceTransformer-bge-base-en-v1.5      CrossEncoder-bge-reranker-base              0.7043    0.7043       0.7043  0.7043  0.8720    0.2180       0.7713  0.7968  0.8872    0.1479       0.7743  0.8026  \n",
      "SentenceTransformer-bge-base-en-v1.5      TinyBERT-MSMARCO                            0.0061    0.0061       0.0061  0.0061  0.0183    0.0046       0.0094  0.0116  0.0305    0.0051       0.0117  0.0162  \n",
      "SentenceTransformer-bge-base-en-v1.5      CrossEncoder-msmarco-distilbert-base-tas-b  0.0335    0.0335       0.0335  0.0335  0.1463    0.0366       0.0757  0.0934  0.2378    0.0396       0.0923  0.1272  \n",
      "SentenceTransformer-multilingual-e5-base  No-Reranker                                 0.5610    0.5610       0.5610  0.5610  0.7866    0.1966       0.6502  0.6846  0.8293    0.1382       0.6583  0.7007  \n",
      "SentenceTransformer-multilingual-e5-base  CrossEncoder-ms-marco-MiniLM-L-12-v2        0.7043    0.7043       0.7043  0.7043  0.8689    0.2172       0.7724  0.7969  0.8902    0.1484       0.7764  0.8049  \n",
      "SentenceTransformer-multilingual-e5-base  CrossEncoder-bge-reranker-base              0.7104    0.7104       0.7104  0.7104  0.8659    0.2165       0.7724  0.7960  0.8902    0.1484       0.7767  0.8049  \n",
      "SentenceTransformer-multilingual-e5-base  TinyBERT-MSMARCO                            0.0000    0.0000       0.0000  0.0000  0.0213    0.0053       0.0053  0.0092  0.0305    0.0051       0.0070  0.0125  \n",
      "SentenceTransformer-multilingual-e5-base  CrossEncoder-msmarco-distilbert-base-tas-b  0.0366    0.0366       0.0366  0.0366  0.1890    0.0473       0.0899  0.1146  0.2866    0.0478       0.1082  0.1512  \n",
      "BM25                                      No-Reranker                                 0.5823    0.5823       0.5823  0.5823  0.8262    0.2066       0.6855  0.7213  0.8628    0.1438       0.6923  0.7350  \n",
      "BM25                                      CrossEncoder-ms-marco-MiniLM-L-12-v2        0.6829    0.6829       0.6829  0.6829  0.8506    0.2127       0.7523  0.7772  0.8750    0.1458       0.7570  0.7865  \n",
      "BM25                                      CrossEncoder-bge-reranker-base              0.7073    0.7073       0.7073  0.7073  0.8506    0.2127       0.7645  0.7862  0.8872    0.1479       0.7714  0.8000  \n",
      "BM25                                      TinyBERT-MSMARCO                            0.0030    0.0030       0.0030  0.0030  0.0152    0.0038       0.0071  0.0091  0.0274    0.0046       0.0093  0.0137  \n",
      "BM25                                      CrossEncoder-msmarco-distilbert-base-tas-b  0.0396    0.0396       0.0396  0.0396  0.1433    0.0358       0.0770  0.0935  0.2317    0.0386       0.0932  0.1264  \n",
      "\n",
      "===== Лучшая комбинация =====\n",
      "Embedding: SentenceTransformer-bge-base-en-v1.5\n",
      "Reranker: CrossEncoder-ms-marco-MiniLM-L-12-v2\n",
      "nDCG@6: 0.8133\n",
      "MRR@6: 0.7836\n",
      "Recall@6: 0.9024\n",
      "Precision@6: 0.1504\n",
      "Финальные результаты сохранены в final_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Функция для вывода всех метрик по каждой модели с разными реранкерами.\"\"\"\n",
    "    # Определяем имена моделей и реранкеров (строки, а не объекты)\n",
    "    embedding_model_names = [\n",
    "        'BAAI/bge-base-en-v1.5',\n",
    "        'intfloat/multilingual-e5-base',\n",
    "        'BM25'  # Специальное значение для BM25\n",
    "    ]\n",
    "    \n",
    "    reranker_names = [\n",
    "        'None',  # Без реранкера\n",
    "        'cross-encoder/ms-marco-MiniLM-L-12-v2',\n",
    "        'BAAI/bge-reranker-base',\n",
    "        'TinyBERT-MSMARCO',  # Специальное значение для TinyBERT\n",
    "        'sentence-transformers/msmarco-distilbert-base-tas-b'\n",
    "    ]\n",
    "    \n",
    "    # Создаем объект DocumentationQA\n",
    "    qa_system = DocumentationQA()\n",
    "    \n",
    "    # Загружаем данные\n",
    "    qa_system.load_data('qdrant_documentation_dataset.csv')\n",
    "    \n",
    "    # Создаем таблицу для итоговых результатов\n",
    "    results_table = []\n",
    "    \n",
    "    # Перебираем модели эмбеддинга\n",
    "    for model_name in embedding_model_names:\n",
    "        # Создаем модель эмбеддинга на основе имени\n",
    "        if model_name == 'BM25':\n",
    "            embedding_model = BM25Model()\n",
    "        else:\n",
    "            try:\n",
    "                # Очистка кэша PyTorch если доступно\n",
    "                import torch\n",
    "                if hasattr(torch, 'cuda') and torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                embedding_model = SentenceTransformerModel(model_name)\n",
    "            except Exception as e:\n",
    "                print(f\"Ошибка при загрузке модели {model_name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Устанавливаем модель и инициализируем базу данных\n",
    "        qa_system.set_embedding_model(embedding_model)\n",
    "        try:\n",
    "            qa_system.initialize_database()\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка при инициализации базы данных с моделью {model_name}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Перебираем реранкеры\n",
    "        for reranker_name in reranker_names:\n",
    "            print(f\"\\n===== Модель: {embedding_model.get_model_name()} + Реранкер: {reranker_name} =====\")\n",
    "            \n",
    "            # Создаем реранкер на основе имени\n",
    "            try:\n",
    "                if reranker_name == 'None':\n",
    "                    reranker = NoReranker()\n",
    "                elif reranker_name == 'TinyBERT-MSMARCO':\n",
    "                    reranker = TinyBertReranker()\n",
    "                else:\n",
    "                    # Очистка кэша PyTorch если доступно\n",
    "                    if hasattr(torch, 'cuda') and torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "                    \n",
    "                    reranker = CrossEncoderReranker(reranker_name)\n",
    "                \n",
    "                # Устанавливаем реранкер\n",
    "                qa_system.set_reranker(reranker)\n",
    "            except Exception as e:\n",
    "                print(f\"Ошибка при загрузке реранкера {reranker_name}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Оцениваем модель с реранкером\n",
    "            try:\n",
    "                first_stage_k = 20  # Для реранкинга берем больше результатов\n",
    "                metrics = qa_system.evaluate_model(k_values=[1, 4, 6], first_stage_k=first_stage_k)\n",
    "                \n",
    "                # Добавляем результат в таблицу\n",
    "                result_row = {\n",
    "                    'Embedding Model': embedding_model.get_model_name(),\n",
    "                    'Reranker': reranker.get_reranker_name()\n",
    "                }\n",
    "                for metric_name, value in metrics.items():\n",
    "                    if metric_name not in ('model_name', 'reranker_name'):\n",
    "                        result_row[metric_name] = value\n",
    "                \n",
    "                results_table.append(result_row)\n",
    "                \n",
    "                # Выводим метрики по группам\n",
    "                print(\"\\nМетрики:\")\n",
    "                \n",
    "                # Recall\n",
    "                print(\"\\nRecall@k:\")\n",
    "                for k in [1, 4, 6]:\n",
    "                    print(f\"  Recall@{k}: {metrics.get(f'Recall@{k}', 0.0):.4f}\")\n",
    "                \n",
    "                # Precision\n",
    "                print(\"\\nPrecision@k:\")\n",
    "                for k in [1, 4, 6]:\n",
    "                    print(f\"  Precision@{k}: {metrics.get(f'Precision@{k}', 0.0):.4f}\")\n",
    "                \n",
    "                # MRR\n",
    "                print(\"\\nMRR@k:\")\n",
    "                for k in [1, 4, 6]:\n",
    "                    print(f\"  MRR@{k}: {metrics.get(f'MRR@{k}', 0.0):.4f}\")\n",
    "                \n",
    "                # nDCG\n",
    "                print(\"\\nnDCG@k:\")\n",
    "                for k in [1, 4, 6]:\n",
    "                    print(f\"  nDCG@{k}: {metrics.get(f'nDCG@{k}', 0.0):.4f}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Ошибка при оценке модели с реранкером {reranker_name}: {e}\")\n",
    "            \n",
    "            # Очистка памяти после каждого теста\n",
    "            if hasattr(torch, 'cuda') and torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "    \n",
    "    # Вывод итоговой сравнительной таблицы\n",
    "    print(\"\\n===== Сводная таблица результатов =====\")\n",
    "    \n",
    "    # Вывод заголовка таблицы\n",
    "    if results_table:\n",
    "        headers = [\"Embedding Model\", \"Reranker\"]\n",
    "        for k in [1, 4, 6]:\n",
    "            for metric in [\"Recall\", \"Precision\", \"MRR\", \"nDCG\"]:\n",
    "                headers.append(f\"{metric}@{k}\")\n",
    "        \n",
    "        # Форматирование ширины столбцов - исправленная версия\n",
    "        col_widths = []\n",
    "        for header in headers:\n",
    "            # Находим максимальную ширину для каждого столбца\n",
    "            max_width = len(header)\n",
    "            for row in results_table:\n",
    "                value = row.get(header, \"\")\n",
    "                value_str = f\"{value:.4f}\" if isinstance(value, float) else str(value)\n",
    "                max_width = max(max_width, len(value_str))\n",
    "            col_widths.append(max_width + 2)  # Добавляем отступ\n",
    "        \n",
    "        # Вывод заголовка\n",
    "        header_row = \"\".join(f\"{header:{width}}\" for header, width in zip(headers, col_widths))\n",
    "        print(header_row)\n",
    "        print(\"-\" * sum(col_widths))\n",
    "        \n",
    "        # Вывод данных\n",
    "        for row in results_table:\n",
    "            row_str = \"\"\n",
    "            for i, header in enumerate(headers):\n",
    "                value = row.get(header, \"\")\n",
    "                if isinstance(value, float):\n",
    "                    row_str += f\"{value:.4f}{' ' * (col_widths[i] - len(f'{value:.4f}'))}\"\n",
    "                else:\n",
    "                    row_str += f\"{value}{' ' * (col_widths[i] - len(str(value)))}\"\n",
    "            print(row_str)\n",
    "    \n",
    "    # Находим лучшую комбинацию по nDCG@6\n",
    "    if results_table:\n",
    "        best_result = max(results_table, key=lambda x: x.get('nDCG@6', 0))\n",
    "        print(\"\\n===== Лучшая комбинация =====\")\n",
    "        print(f\"Embedding: {best_result['Embedding Model']}\")\n",
    "        print(f\"Reranker: {best_result['Reranker']}\")\n",
    "        print(f\"nDCG@6: {best_result.get('nDCG@6', 0):.4f}\")\n",
    "        print(f\"MRR@6: {best_result.get('MRR@6', 0):.4f}\")\n",
    "        print(f\"Recall@6: {best_result.get('Recall@6', 0):.4f}\")\n",
    "        print(f\"Precision@6: {best_result.get('Precision@6', 0):.4f}\")\n",
    "        \n",
    "    # Сохраняем результаты в файл\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        pd.DataFrame(results_table).to_csv('final_results.csv', index=False)\n",
    "        print(\"Финальные результаты сохранены в final_results.csv\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при сохранении финальных результатов: {e}\")\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наилучшая комбинация - модель SentenceTransformer('BAAI/bge-base-en-v1.5') с реранкером cross-encoder/ms-marco-MiniLM-L-12-v2. Однако вижу большой разрыв между Recall@1 и Recall@4. Возможно будет лучше выбрать 2 или три фрагмента. Расчитаю дополнительные метрики для этой комбинации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружено 328 записей из датасета.\n",
      "Найдено 121 уникальных чанков для индексации.\n",
      "Вычисление эмбеддингов для батча 1/2\n",
      "Вычисление эмбеддингов для батча 2/2\n",
      "База данных успешно инициализирована с моделью SentenceTransformer-bge-base-en-v1.5.\n",
      "Расчет дополнительных метрик для: SentenceTransformer-bge-base-en-v1.5 с реранкером: CrossEncoder-ms-marco-MiniLM-L-12-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Расчет метрик @2 и @3:   0%|          | 0/328 [00:00<?, ?it/s]C:\\Users\\sekho\\AppData\\Local\\Temp\\ipykernel_25372\\1846738923.py:73: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_result = self.client.search(\n",
      "Расчет метрик @2 и @3: 100%|██████████| 328/328 [15:00<00:00,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Дополнительные метрики:\n",
      "\n",
      "Метрики для k=2:\n",
      "  Recall@2: 0.7988\n",
      "  Precision@2: 0.3994\n",
      "  MRR@2: 0.7546\n",
      "  nDCG@2: 0.7661\n",
      "\n",
      "Метрики для k=3:\n",
      "  Recall@3: 0.8537\n",
      "  Precision@3: 0.2846\n",
      "  MRR@3: 0.7729\n",
      "  nDCG@3: 0.7936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Вычисление дополнительных метрик @2 и @3 для конкретной комбинации модели и реранкера\n",
    "def calculate_additional_metrics():\n",
    "    # Создаем объекты модели и реранкера\n",
    "    embedding_model = SentenceTransformerModel('BAAI/bge-base-en-v1.5')\n",
    "    reranker = CrossEncoderReranker('cross-encoder/ms-marco-MiniLM-L-12-v2')\n",
    "    \n",
    "    # Создаем QA систему с выбранной моделью и реранкером\n",
    "    qa_system = DocumentationQA(embedding_model=embedding_model, reranker=reranker)\n",
    "    \n",
    "    # Загружаем данные\n",
    "    qa_system.load_data('qdrant_documentation_dataset.csv')\n",
    "    \n",
    "    # Инициализируем базу данных (если она еще не инициализирована)\n",
    "    if not qa_system.is_initialized:\n",
    "        qa_system.initialize_database()\n",
    "    \n",
    "    # Метрики будем считать только для k=2 и k=3\n",
    "    k_values = [2, 3]\n",
    "    first_stage_k = 20  # Для реранкинга берем больше результатов\n",
    "    \n",
    "    # Получаем все метрики\n",
    "    all_metrics = []\n",
    "    model_name = embedding_model.get_model_name()\n",
    "    reranker_name = reranker.get_reranker_name()\n",
    "    \n",
    "    print(f\"Расчет дополнительных метрик для: {model_name} с реранкером: {reranker_name}\")\n",
    "    \n",
    "    # Перебираем все вопросы в датасете\n",
    "    for idx, row in tqdm(qa_system.df.iterrows(), total=len(qa_system.df), desc=\"Расчет метрик @2 и @3\"):\n",
    "        question = row['question']\n",
    "        relevant_section = row['section_content']\n",
    "        relevant_id = qa_system.section_id_map[relevant_section]\n",
    "        \n",
    "        # Получаем результаты поиска с реранкингом\n",
    "        search_results = qa_system.search_similar_sections(\n",
    "            question, \n",
    "            top_k=max(k_values), \n",
    "            first_stage_k=first_stage_k\n",
    "        )\n",
    "        retrieved_ids = [result['id'] for result in search_results]\n",
    "        \n",
    "        # Рассчитываем метрики для текущего запроса\n",
    "        query_metrics = {}\n",
    "        for k in k_values:\n",
    "            query_metrics[f'Recall@{k}'] = qa_system.metrics_calculator.calculate_recall_at_k(\n",
    "                [relevant_id], retrieved_ids, k)\n",
    "            query_metrics[f'Precision@{k}'] = qa_system.metrics_calculator.calculate_precision_at_k(\n",
    "                [relevant_id], retrieved_ids, k)\n",
    "            query_metrics[f'MRR@{k}'] = qa_system.metrics_calculator.calculate_mrr_at_k(\n",
    "                [relevant_id], retrieved_ids, k)\n",
    "            query_metrics[f'nDCG@{k}'] = qa_system.metrics_calculator.calculate_ndcg_at_k(\n",
    "                [relevant_id], retrieved_ids, k)\n",
    "        \n",
    "        all_metrics.append(query_metrics)\n",
    "    \n",
    "    # Вычисляем средние метрики\n",
    "    average_metrics = qa_system.metrics_calculator.compute_average_metrics(all_metrics)\n",
    "    \n",
    "    # Выводим метрики\n",
    "    print(\"\\nДополнительные метрики:\")\n",
    "    for k in k_values:\n",
    "        print(f\"\\nМетрики для k={k}:\")\n",
    "        print(f\"  Recall@{k}: {average_metrics.get(f'Recall@{k}', 0.0):.4f}\")\n",
    "        print(f\"  Precision@{k}: {average_metrics.get(f'Precision@{k}', 0.0):.4f}\")\n",
    "        print(f\"  MRR@{k}: {average_metrics.get(f'MRR@{k}', 0.0):.4f}\")\n",
    "        print(f\"  nDCG@{k}: {average_metrics.get(f'nDCG@{k}', 0.0):.4f}\")\n",
    "    \n",
    "    return average_metrics\n",
    "\n",
    "# Выполняем расчет\n",
    "additional_metrics = calculate_additional_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итого, по Recall@3 выберем три фрагмента."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
