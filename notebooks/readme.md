## Ноутбук topic_classification

Как и было указано, в нашей документации есть 11 разделов. Для нашей модели важно, к какой теме принадлежит вопрос для корректного ответа, однако стоит учитывать, что вопрос пользователя может принадлежать к нескольким темам сразу. 

Предложенная модель представляет собой многоклассовый классификатор для определения тематик вопросов, основанный на архитектуре BERT с дополнительными модификациями для повышения точности предсказаний.

# Архитектура модели состоит из следующих основных компонентов:

Энкодер на базе RuBERT (DeepPavlov/rubert-base-cased)
Используется предобученная языковая модель для русского языка
Применяется техника частичной заморозки параметров: активными остаются только последние два слоя трансформера, что позволяет сфокусировать обучение на высокоуровневых признаках при сохранении базовых языковых представлений
Механизм внимания (TopicAttention)
Реализует самовнимание над конкатенацией выходов последних четырех слоев BERT
Позволяет модели динамически взвешивать важность различных семантических признаков текста
Использует нелинейное преобразование с уменьшением размерности для вычисления весов внимания
Классификационная головка
Состоит из трех полносвязных слоев с промежуточной нормализацией и GELU активацией
Включает dropout (0.3) для регуляризации
Постепенно уменьшает размерность для получения финального вектора предсказаний

# Особенности обучения:

Кастомная функция потерь (CustomLoss), объединяющая три компонента:
Базовая бинарная кросс-энтропия для многометочной классификации
Штрафной член за превышение максимально допустимого количества предсказанных классов
Штраф за отсутствие истинных меток среди топ-k предсказаний
Дифференцированное обучение параметров:
Повышенный learning rate для слоев внимания и классификатора
Пониженный learning rate для настройки слоев BERT
Применение линейного планировщика с разогревом
Контроль предсказаний:
Жесткое ограничение на максимальное количество предсказываемых классов (не более трех)
Использование top-k механизма выбора наиболее вероятных классов
Оптимизация с учетом необходимости включения истинного класса в предсказания

# Процесс инференса:

Текст вопроса токенизируется и подается на вход модели
Последовательно применяются:
Обработка через BERT-энкодер
Агрегация выходов последних слоев
Механизм внимания
Классификационные слои
Выходные логиты преобразуются в вероятности через сигмоиду
Выбираются top-3 класса с наивысшими вероятностями

# Модель оптимизирована для решения специфической задачи определения тематик вопросов с учетом следующих требований:

Ограничение количества предсказываемых тематик
Максимизация вероятности включения истинной тематики в список предсказанных
Устойчивость к шуму и неоднозначности в формулировках вопросов
Данная архитектура представляет собой баланс между сложностью модели и её способностью к обобщению, что особенно важно при работе с ограниченным набором обучающих данных в условиях многометочной классификации.
