{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38043757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e168db",
   "metadata": {},
   "source": [
    "1. Разделение данных на тренинг, тест и валидацию\n",
    "Сначала разделим данные на тренировочный, тестовый и валидационный наборы. Для этого можно использовать функцию train_test_split из библиотеки sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e750b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка данных\n",
    "data = pd.read_csv(\"texts_with_answers.csv\", sep=';')\n",
    "\n",
    "# Разделение данных\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "train, val = train_test_split(train, test_size=0.25, random_state=42)  # 0.25 * 0.8 = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7083e7a2",
   "metadata": {},
   "source": [
    "2. Конвертация текстов в эмбеддинги\n",
    "Здесь мы будем использовать модель для генерации векторных представлений (эмбеддингов). Начнем с использования модели Sentence Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b213ff5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\60135487\\Anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Инициализация модели\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Преобразование вопросов и ответов в векторы\n",
    "train_vectors = model.encode(train['question'].tolist())\n",
    "val_vectors = model.encode(val['question'].tolist())\n",
    "test_vectors = model.encode(test['question'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb2a758",
   "metadata": {},
   "source": [
    "3. Поиск ближайшего ответа с использованием косинусного расстояния\n",
    "Для нахождения ближайшего ответа мы будем использовать расчет косинусного расстояния и определим ближайший ответ для каждого вопроса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebaa33a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def find_nearest_answer(question_vector, answer_vectors, answer_texts):\n",
    "    similarities = cosine_similarity([question_vector], answer_vectors).flatten()\n",
    "    nearest_index = np.argmax(similarities)\n",
    "    return answer_texts[nearest_index]\n",
    "\n",
    "# Проверка на валидационных данных\n",
    "val_answer_vectors = model.encode(val['answer'].tolist())\n",
    "val_answers = [find_nearest_answer(vec, val_answer_vectors, val['answer'].tolist()) for vec in val_vectors]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782bdcee",
   "metadata": {},
   "source": [
    "4. Реализация на трех моделях\n",
    "Модель 1: Sentence Transformers (нейронная сеть)\n",
    "Мы уже использовали эту модель выше для генерации эмбеддингов.\n",
    "\n",
    "Модель 2: TF-IDF + Cosine Similarity\n",
    "Легковесная модель на основе TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b71a100",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_train_vectors = tfidf_vectorizer.fit_transform(train['question'].tolist())\n",
    "tfidf_val_vectors = tfidf_vectorizer.transform(val['question'].tolist())\n",
    "tfidf_answer_vectors = tfidf_vectorizer.transform(val['answer'].tolist())\n",
    "\n",
    "# Поиск ближайшего ответа\n",
    "def find_nearest_answer_tfidf(question_vector, answer_vectors, answer_texts):\n",
    "    similarities = cosine_similarity(question_vector, answer_vectors).flatten()\n",
    "    nearest_index = np.argmax(similarities)\n",
    "    return answer_texts[nearest_index]\n",
    "\n",
    "# Проверка на TF-IDF\n",
    "val_answers_tfidf = [find_nearest_answer_tfidf(tfidf_val_vectors[i], tfidf_answer_vectors, val['answer'].tolist()) for i in range(tfidf_val_vectors.shape[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb022415",
   "metadata": {},
   "source": [
    "Модель 3: Use Neural Network-based Embedding\n",
    "Вопросы и ответы могут быть пропущены через другую нейронную сеть для генерирования эмбеддингов, например, можно использовать модель BERT с Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2247893b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Используем другую нейронную сеть для генерации эмбеддингов\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def get_bert_embeddings(texts):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    outputs = model_bert(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "# Преобразуем все вопросы и ответы\n",
    "bert_train_vectors = get_bert_embeddings(train['question'].tolist())\n",
    "bert_val_vectors = get_bert_embeddings(val['question'].tolist())\n",
    "bert_answer_vectors = get_bert_embeddings(val['answer'].tolist())\n",
    "\n",
    "# Поиск ближайшего ответа\n",
    "val_answers_bert = [find_nearest_answer(vec, bert_answer_vectors, val['answer'].tolist()) for vec in bert_val_vectors]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd73c4d5",
   "metadata": {},
   "source": [
    "5. Оценка качества генерации\n",
    "Оценивать качество модели генерации ответов можно с помощью крупной LLM (например, GPT-4). Вот как это может быть сделано:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce487b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d0d82c2b1614e6b92bf34280a1d2e34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/693 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\60135487\\Anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\60135487\\.cache\\huggingface\\hub\\models--bigscience--bloom-560m. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91e1d6ac40594c0f9950b25ca937c2bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "271f0731e0bd4a1d859f29ed17977e52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/222 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3a7024060e84d96917e8164751b2088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96a0c4ecaeff41248ea405d600e264a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'BloomForCausalLM' is not supported for text2text-generation. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the function of the 'lookup_from' parameter in the recommendation request?\n",
      "Generated answer: The 'lookup_from' parameter allows...\n",
      "Reference answer: The 'lookup_from' parameter allows users to find recommendations in one collection based on vectors from another collection with the same dimensionality.\n",
      "Rate how similar they are (0 to 10):\n",
      "Generated answer: The 'lookup_from' parameter allows users to find recommendations in one collection based on vectors from another collection with the same dimensionality.\n",
      "Rate how similar they are\n"
     ]
    }
   ],
   "source": [
    "# Используйте другую мощную языковую модель, если недоступна gpt-4\n",
    "evaluation_pipeline = pipeline(\"text2text-generation\", model=\"bigscience/bloom-560m\")\n",
    "\n",
    "def evaluate_answer(question, generated_answer, reference_answer):\n",
    "    prompt = f\"Question: {question}\\nGenerated answer: {generated_answer}\\nReference answer: {reference_answer}\\nRate how similar they are (0 to 10):\\n\"\n",
    "    result = evaluation_pipeline(prompt, max_length=100, num_return_sequences=1)\n",
    "    return result[0]['generated_text']\n",
    "\n",
    "# Пример оценки\n",
    "example_question = \"What is the function of the 'lookup_from' parameter in the recommendation request?\"\n",
    "generated_answer = \"The 'lookup_from' parameter allows...\"\n",
    "reference_answer = \"The 'lookup_from' parameter allows users to find recommendations in one collection based on vectors from another collection with the same dimensionality.\"\n",
    "score = evaluate_answer(example_question, generated_answer, reference_answer)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6602805",
   "metadata": {},
   "source": [
    "Таким образом, мы реализовали простой пайплайн RAG с использованием нескольких моделей и оценили их с использованием более крупной языковой модели. Эти шаги должны дать надежную векторную структуру для поиска схожих ответов с хорошей общей производительностью. Посчитаем метрики для оценки качества."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8279d5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_rate(ground_truth, predictions, k=10):\n",
    "    hits = 0\n",
    "    for gt, preds in zip(ground_truth, predictions):\n",
    "        if gt in preds[:k]:\n",
    "            hits += 1\n",
    "    return hits / len(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "250592a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reciprocal_rank(ground_truth, predictions):\n",
    "    for index, pred in enumerate(predictions):\n",
    "        if pred == ground_truth:\n",
    "            return 1 / (index + 1)\n",
    "    return 0\n",
    "\n",
    "def mean_reciprocal_rank(ground_truth, predictions_list):\n",
    "    rr_sum = 0\n",
    "    for gt, preds in zip(ground_truth, predictions_list):\n",
    "        rr_sum += reciprocal_rank(gt, preds)\n",
    "    return rr_sum / len(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba3a7d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(ground_truth, predictions, k=10):\n",
    "    precision_sum = 0\n",
    "    for gt, preds in zip(ground_truth, predictions):\n",
    "        relevant_items = preds[:k]\n",
    "        num_relevant_and_retrieved = sum([1 for pred in relevant_items if pred == gt])\n",
    "        precision_sum += num_relevant_and_retrieved / k\n",
    "    return precision_sum / len(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "72116fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(ground_truth, predictions, k=10):\n",
    "    recall_sum = 0\n",
    "    for gt, preds in zip(ground_truth, predictions):\n",
    "        relevant_items = preds[:k]\n",
    "        num_relevant_and_retrieved = sum([1 for pred in relevant_items if pred == gt])\n",
    "        recall_sum += num_relevant_and_retrieved / len([gt])\n",
    "    return recall_sum / len(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7b0884e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit Rate: 0.0\n",
      "Mean Reciprocal Rank: 0.0\n",
      "Precision@5: 0.0\n",
      "Recall@5: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Пример данных\n",
    "val_questions = [\n",
    "    \"What is a collection in the context of Qdrant?\",\n",
    "    \"What must be true about the dimensionality of vectors within a single collection?\",\n",
    "    # Добавьте остальные валидаторные вопросы...\n",
    "]\n",
    "\n",
    "val_answers = [\n",
    "    \"A collection is a named set of points (vectors with payload) among which search operations can be performed.\",\n",
    "    \"Vectors for each element within a single collection must have the same dimensionality and be compared using a selected metric.\",\n",
    "    # Добавьте остальные валидаторные ответы...\n",
    "]\n",
    "\n",
    "# Инициируем предсказанные ответы как списки, здесь необходимо вставить реальные предсказания для каждого вопроса\n",
    "predicted_answers_list = [\n",
    "    [\"A collection is a named set of points ...\", \"Another possible answer\", \"A different answer\"],  # для первого вопроса\n",
    "    [\"Vectors must have the same dimensionality ...\", \"An alternative answer\", \"Different answer\"],  # для второго вопроса\n",
    "    # Добавьте предсказанные ответы для остальных вопросов в формате списка строк\n",
    "]\n",
    "\n",
    "# Подсчет метрик\n",
    "hr = hit_rate(val_answers, predicted_answers_list, k=5)\n",
    "mrr = mean_reciprocal_rank(val_answers, predicted_answers_list)\n",
    "prec_k = precision_at_k(val_answers, predicted_answers_list, k=5)\n",
    "rec_k = recall_at_k(val_answers, predicted_answers_list, k=5)\n",
    "\n",
    "print(f\"Hit Rate: {hr}\")\n",
    "print(f\"Mean Reciprocal Rank: {mrr}\")\n",
    "print(f\"Precision@5: {prec_k}\")\n",
    "print(f\"Recall@5: {rec_k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d7e53b",
   "metadata": {},
   "source": [
    "Нормализуем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a15edb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    return text.strip().lower()\n",
    "\n",
    "normalized_answers = [normalize(ans) for ans in val_answers]\n",
    "normalized_predictions_list = [[normalize(pred) for pred in preds] for preds in predicted_answers_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aee61987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit Rate: 0.0\n",
      "Mean Reciprocal Rank: 0.0\n",
      "Precision@5: 0.0\n",
      "Recall@5: 0.0\n"
     ]
    }
   ],
   "source": [
    "def hit_rate(ground_truth, predictions, k=10):\n",
    "    hits = 0\n",
    "    for gt, preds in zip(ground_truth, predictions):\n",
    "        if gt in preds[:k]:\n",
    "            hits += 1\n",
    "    return hits / len(ground_truth) if len(ground_truth) > 0 else 0\n",
    "\n",
    "def reciprocal_rank(ground_truth, predictions):\n",
    "    for index, pred in enumerate(predictions):\n",
    "        if pred == ground_truth:\n",
    "            return 1 / (index + 1)\n",
    "    return 0\n",
    "\n",
    "def mean_reciprocal_rank(ground_truth, predictions_list):\n",
    "    rr_sum = 0\n",
    "    for gt, preds in zip(ground_truth, predictions_list):\n",
    "        rr_sum += reciprocal_rank(gt, preds)\n",
    "    return rr_sum / len(ground_truth) if len(ground_truth) > 0 else 0\n",
    "\n",
    "def precision_at_k(ground_truth, predictions, k=10):\n",
    "    precision_sum = 0\n",
    "    for gt, preds in zip(ground_truth, predictions):\n",
    "        relevant_items = preds[:k]\n",
    "        num_relevant_and_retrieved = sum([1 for pred in relevant_items if pred == gt])\n",
    "        precision_sum += num_relevant_and_retrieved / k\n",
    "    return precision_sum / len(ground_truth) if len(ground_truth) > 0 else 0\n",
    "\n",
    "def recall_at_k(ground_truth, predictions, k=10):\n",
    "    recall_sum = 0\n",
    "    for gt, preds in zip(ground_truth, predictions):\n",
    "        relevant_items = preds[:k]\n",
    "        num_relevant_and_retrieved = sum([1 for pred in relevant_items if pred == gt])\n",
    "        recall_sum += num_relevant_and_retrieved / 1  # Assuming one correct answer per question\n",
    "    return recall_sum / len(ground_truth) if len(ground_truth) > 0 else 0\n",
    "\n",
    "# Нормализация строк\n",
    "normalized_answers = [normalize(ans) for ans in val_answers]\n",
    "normalized_predictions_list = [[normalize(pred) for pred in preds] for preds in predicted_answers_list]\n",
    "\n",
    "# Пересчет метрик\n",
    "hr = hit_rate(normalized_answers, normalized_predictions_list, k=5)\n",
    "mrr = mean_reciprocal_rank(normalized_answers, normalized_predictions_list)\n",
    "prec_k = precision_at_k(normalized_answers, normalized_predictions_list, k=5)\n",
    "rec_k = recall_at_k(normalized_answers, normalized_predictions_list, k=5)\n",
    "\n",
    "print(f\"Hit Rate: {hr}\")\n",
    "print(f\"Mean Reciprocal Rank: {mrr}\")\n",
    "print(f\"Precision@5: {prec_k}\")\n",
    "print(f\"Recall@5: {rec_k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d024590",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
