{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6cbad218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional, Union\n",
    "from tqdm import tqdm\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Make sure we have the necessary NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "class HybridSearchManager:\n",
    "    \"\"\"Manages hybrid search using Qdrant Query API.\"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name: str = \"documentation\"):\n",
    "        self.client = QdrantClient(\":memory:\")  # In-memory database for testing\n",
    "        self.collection_name = collection_name\n",
    "        self.dense_model = SentenceTransformer('BAAI/bge-large-en-v1.5')\n",
    "        self.vector_size = self.dense_model.get_sentence_embedding_dimension()\n",
    "        self.corpus = []\n",
    "        self.id_to_idx = {}\n",
    "        \n",
    "        # For sparse vectors\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.tfidf_vectorizer = None\n",
    "    \n",
    "    def preprocess_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Tokenize, remove stopwords, and stem the text.\"\"\"\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        tokens = [self.stemmer.stem(token) for token in tokens \n",
    "                  if token.isalnum() and token not in self.stop_words]\n",
    "        return tokens\n",
    "    \n",
    "    def initialize_collection(self):\n",
    "        \"\"\"Initialize the Qdrant collection with both dense and sparse vector support.\"\"\"\n",
    "        # Delete existing collection if it exists\n",
    "        try:\n",
    "            self.client.delete_collection(collection_name=self.collection_name)\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        # Create new collection with dense vector support\n",
    "        self.client.create_collection(\n",
    "            collection_name=self.collection_name,\n",
    "            vectors_config={\n",
    "                \"dense\": models.VectorParams(\n",
    "                    size=self.vector_size,\n",
    "                    distance=models.Distance.COSINE\n",
    "                ),\n",
    "                \"sparse\": models.VectorParams(\n",
    "                    size=10000,  # Maximum sparse vector size\n",
    "                    distance=models.Distance.COSINE  # Using COSINE instead of Dot\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Reset the corpus\n",
    "        self.corpus = []\n",
    "        self.id_to_idx = {}\n",
    "    \n",
    "    def create_sparse_vector_as_dense(self, text: str) -> List[float]:\n",
    "        \"\"\"Create a sparse vector as dense format for compatibility.\"\"\"\n",
    "        if self.tfidf_vectorizer is None:\n",
    "            raise ValueError(\"TF-IDF vectorizer not initialized. Call compute_tfidf_vectorizer first.\")\n",
    "        \n",
    "        # Preprocess the text\n",
    "        preprocessed_text = ' '.join(self.preprocess_text(text))\n",
    "        \n",
    "        # Transform the text using the pre-trained TF-IDF vectorizer\n",
    "        vector = self.tfidf_vectorizer.transform([preprocessed_text])\n",
    "        \n",
    "        # Convert to dense list with sparse values\n",
    "        sparse_vector = [0.0] * self.tfidf_vectorizer.max_features\n",
    "        for idx, value in zip(vector.indices, vector.data):\n",
    "            sparse_vector[idx] = value\n",
    "        \n",
    "        # Normalize the vector for cosine similarity\n",
    "        norm = np.linalg.norm(sparse_vector)\n",
    "        if norm > 0:\n",
    "            sparse_vector = [v / norm for v in sparse_vector]\n",
    "        \n",
    "        return sparse_vector\n",
    "    \n",
    "    def compute_tfidf_vectorizer(self, texts: List[str]):\n",
    "        \"\"\"Compute the TF-IDF vectorizer based on the corpus.\"\"\"\n",
    "        # Preprocess all texts\n",
    "        preprocessed_texts = [' '.join(self.preprocess_text(text)) for text in texts]\n",
    "        \n",
    "        # Create and fit the TF-IDF vectorizer\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(max_features=10000)\n",
    "        self.tfidf_vectorizer.fit(preprocessed_texts)\n",
    "    \n",
    "    def upsert_batch(self, id_offset: int, texts: List[str], payloads: List[Dict[str, Any]]):\n",
    "        \"\"\"Add documents to Qdrant with both dense and sparse vectors.\"\"\"\n",
    "        # Save texts in corpus\n",
    "        for idx, text in enumerate(texts):\n",
    "            corpus_idx = len(self.corpus)\n",
    "            self.corpus.append(text)\n",
    "            self.id_to_idx[idx + id_offset] = corpus_idx\n",
    "        \n",
    "        # Compute TF-IDF vectorizer if not already computed\n",
    "        if self.tfidf_vectorizer is None:\n",
    "            self.compute_tfidf_vectorizer(texts)\n",
    "        \n",
    "        # Compute dense embeddings\n",
    "        dense_embeddings = self.dense_model.encode(texts, normalize_embeddings=True)\n",
    "        \n",
    "        # Prepare points for upserting\n",
    "        points = []\n",
    "        for idx, (text, payload, dense_embedding) in enumerate(zip(texts, payloads, dense_embeddings)):\n",
    "            # Create sparse vector as dense format\n",
    "            sparse_vector = self.create_sparse_vector_as_dense(text)\n",
    "            \n",
    "            # Create point with both vectors\n",
    "            points.append(models.PointStruct(\n",
    "                id=idx + id_offset,\n",
    "                vector={\n",
    "                    \"dense\": dense_embedding.tolist(),\n",
    "                    \"sparse\": sparse_vector\n",
    "                },\n",
    "                payload=payload\n",
    "            ))\n",
    "        \n",
    "        # Upsert points\n",
    "        self.client.upsert(\n",
    "            collection_name=self.collection_name,\n",
    "            points=points\n",
    "        )\n",
    "    \n",
    "    def hybrid_search(self, query: str, limit: int = 6, hybrid_method: str = \"fusion\") -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Perform hybrid search using Qdrant's Query API.\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            limit: Number of results to return\n",
    "            hybrid_method: Method to use for hybrid search.\n",
    "                Options:\n",
    "                - \"fusion\": Combine dense and sparse using RRF fusion\n",
    "                - \"dense_rerank\": First get sparse candidates, then rerank with dense\n",
    "                - \"sparse_rerank\": First get dense candidates, then rerank with sparse\n",
    "        \n",
    "        Returns:\n",
    "            List of search results\n",
    "        \"\"\"\n",
    "        dense_vector = self.dense_model.encode(query, normalize_embeddings=True).tolist()\n",
    "        sparse_vector = self.create_sparse_vector_as_dense(query)\n",
    "        \n",
    "        if hybrid_method == \"fusion\":\n",
    "            # For fusion, get results from both vectors separately and combine them\n",
    "            try:\n",
    "                dense_results = self.client.search(\n",
    "                    collection_name=self.collection_name,\n",
    "                    query_vector=(\"dense\", dense_vector),\n",
    "                    limit=limit * 2\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error in dense search: {e}\")\n",
    "                dense_results = []\n",
    "            \n",
    "            try:\n",
    "                sparse_results = self.client.search(\n",
    "                    collection_name=self.collection_name,\n",
    "                    query_vector=(\"sparse\", sparse_vector),\n",
    "                    limit=limit * 2\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error in sparse search: {e}\")\n",
    "                sparse_results = []\n",
    "            \n",
    "            # Simple RRF fusion (Reciprocal Rank Fusion)\n",
    "            dense_scores = {hit.id: 1.0/(1.0 + idx) for idx, hit in enumerate(dense_results)}\n",
    "            sparse_scores = {hit.id: 1.0/(1.0 + idx) for idx, hit in enumerate(sparse_results)}\n",
    "            \n",
    "            # Combine all document IDs\n",
    "            all_ids = set(dense_scores.keys()) | set(sparse_scores.keys())\n",
    "            \n",
    "            if not all_ids:\n",
    "                return []\n",
    "            \n",
    "            # Calculate combined scores\n",
    "            combined_scores = []\n",
    "            for doc_id in all_ids:\n",
    "                dense_score = dense_scores.get(doc_id, 0.0)\n",
    "                sparse_score = sparse_scores.get(doc_id, 0.0)\n",
    "                combined_score = dense_score + sparse_score\n",
    "                combined_scores.append((doc_id, combined_score))\n",
    "            \n",
    "            # Sort by combined score\n",
    "            combined_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            # Get top results\n",
    "            top_ids = [doc_id for doc_id, _ in combined_scores[:limit]]\n",
    "            \n",
    "            if not top_ids:\n",
    "                return []\n",
    "            \n",
    "            # Retrieve points\n",
    "            try:\n",
    "                results = self.client.retrieve(\n",
    "                    collection_name=self.collection_name,\n",
    "                    ids=top_ids\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error retrieving points: {e}\")\n",
    "                return []\n",
    "            \n",
    "            # Map to common format with score\n",
    "            search_results = []\n",
    "            for point in results:\n",
    "                score = next((score for id, score in combined_scores if id == point.id), 0.0)\n",
    "                search_results.append({\n",
    "                    'text': point.payload.get('text', ''),\n",
    "                    'id': point.payload.get('id', point.id),\n",
    "                    'score': score\n",
    "                })\n",
    "            \n",
    "            # Sort by score\n",
    "            search_results.sort(key=lambda x: x['score'], reverse=True)\n",
    "        \n",
    "        elif hybrid_method == \"dense_rerank\":\n",
    "            # First get sparse candidates\n",
    "            try:\n",
    "                sparse_results = self.client.search(\n",
    "                    collection_name=self.collection_name,\n",
    "                    query_vector=(\"sparse\", sparse_vector),\n",
    "                    limit=limit * 3\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error in sparse search: {e}\")\n",
    "                return []\n",
    "            \n",
    "            sparse_ids = [hit.id for hit in sparse_results]\n",
    "            \n",
    "            if not sparse_ids:\n",
    "                return []\n",
    "            \n",
    "            # Retrieve full points for candidates\n",
    "            try:\n",
    "                candidate_points = self.client.retrieve(\n",
    "                    collection_name=self.collection_name,\n",
    "                    ids=sparse_ids\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error retrieving points: {e}\")\n",
    "                return []\n",
    "            \n",
    "            # Manual scoring with dense vector\n",
    "            dense_scores = []\n",
    "            for point in candidate_points:\n",
    "                # Skip points without the required vector\n",
    "                if not hasattr(point, 'vector') or point.vector is None or \"dense\" not in point.vector:\n",
    "                    continue\n",
    "                    \n",
    "                try:\n",
    "                    # Calculate cosine similarity manually\n",
    "                    dense_vec = np.array(point.vector[\"dense\"])\n",
    "                    if dense_vec is None:\n",
    "                        continue\n",
    "                        \n",
    "                    dense_query_vec = np.array(dense_vector)\n",
    "                    \n",
    "                    # Normalize if needed\n",
    "                    norm_dense = np.linalg.norm(dense_vec)\n",
    "                    if norm_dense > 0:\n",
    "                        dense_vec = dense_vec / norm_dense\n",
    "                        \n",
    "                    norm_query = np.linalg.norm(dense_query_vec)\n",
    "                    if norm_query > 0:\n",
    "                        dense_query_vec = dense_query_vec / norm_query\n",
    "                    \n",
    "                    # Calculate cosine similarity\n",
    "                    similarity = np.dot(dense_vec, dense_query_vec)\n",
    "                    \n",
    "                    dense_scores.append((point, similarity))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error calculating similarity: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Sort by dense score\n",
    "            dense_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            # Format results\n",
    "            search_results = [\n",
    "                {\n",
    "                    'text': point.payload.get('text', ''),\n",
    "                    'id': point.payload.get('id', point.id),\n",
    "                    'score': score\n",
    "                }\n",
    "                for point, score in dense_scores[:limit]\n",
    "            ]\n",
    "        \n",
    "        elif hybrid_method == \"sparse_rerank\":\n",
    "            # Dense first\n",
    "            try:\n",
    "                dense_results = self.client.search(\n",
    "                    collection_name=self.collection_name,\n",
    "                    query_vector=(\"dense\", dense_vector),\n",
    "                    limit=limit * 3\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error in dense search: {e}\")\n",
    "                return []\n",
    "            \n",
    "            dense_ids = [hit.id for hit in dense_results]\n",
    "            \n",
    "            if not dense_ids:\n",
    "                return []\n",
    "            \n",
    "            # Retrieve full points for candidates\n",
    "            try:\n",
    "                candidate_points = self.client.retrieve(\n",
    "                    collection_name=self.collection_name,\n",
    "                    ids=dense_ids\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error retrieving points: {e}\")\n",
    "                return []\n",
    "            \n",
    "            # Manual scoring with sparse vector\n",
    "            sparse_scores = []\n",
    "            for point in candidate_points:\n",
    "                # Skip points without the required vector\n",
    "                if not hasattr(point, 'vector') or point.vector is None or \"sparse\" not in point.vector:\n",
    "                    continue\n",
    "                    \n",
    "                try:\n",
    "                    # Calculate cosine similarity manually\n",
    "                    sparse_vec = np.array(point.vector[\"sparse\"])\n",
    "                    if sparse_vec is None:\n",
    "                        continue\n",
    "                        \n",
    "                    sparse_query_vec = np.array(sparse_vector)\n",
    "                    \n",
    "                    # Normalize if needed\n",
    "                    norm_sparse = np.linalg.norm(sparse_vec)\n",
    "                    if norm_sparse > 0:\n",
    "                        sparse_vec = sparse_vec / norm_sparse\n",
    "                        \n",
    "                    norm_query = np.linalg.norm(sparse_query_vec)\n",
    "                    if norm_query > 0:\n",
    "                        sparse_query_vec = sparse_query_vec / norm_query\n",
    "                    \n",
    "                    # Calculate cosine similarity\n",
    "                    similarity = np.dot(sparse_vec, sparse_query_vec)\n",
    "                    \n",
    "                    sparse_scores.append((point, similarity))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error calculating similarity: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Sort by sparse score\n",
    "            sparse_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            # Format results\n",
    "            search_results = [\n",
    "                {\n",
    "                    'text': point.payload.get('text', ''),\n",
    "                    'id': point.payload.get('id', point.id),\n",
    "                    'score': score\n",
    "                }\n",
    "                for point, score in sparse_scores[:limit]\n",
    "            ]\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Invalid hybrid method: {hybrid_method}\")\n",
    "        \n",
    "        return search_results\n",
    "    \n",
    "    def weighted_hybrid_search(self, query: str, dense_weight: float = 0.5, \n",
    "                              sparse_weight: float = 0.5, limit: int = 6) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Perform weighted hybrid search using a custom scoring function.\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            dense_weight: Weight for dense vector scores (0.0 to 1.0)\n",
    "            sparse_weight: Weight for sparse vector scores (0.0 to 1.0)\n",
    "            limit: Number of results to return\n",
    "            \n",
    "        Returns:\n",
    "            List of search results with custom-weighted scores\n",
    "        \"\"\"\n",
    "        if not (0 <= dense_weight <= 1) or not (0 <= sparse_weight <= 1):\n",
    "            raise ValueError(\"Weights must be between 0 and 1\")\n",
    "        \n",
    "        # Normalize weights to sum to 1.0\n",
    "        total_weight = dense_weight + sparse_weight\n",
    "        if total_weight == 0:\n",
    "            raise ValueError(\"At least one weight must be positive\")\n",
    "        \n",
    "        dense_weight = dense_weight / total_weight\n",
    "        sparse_weight = sparse_weight / total_weight\n",
    "        \n",
    "        # Encode query\n",
    "        dense_vector = self.dense_model.encode(query, normalize_embeddings=True).tolist()\n",
    "        sparse_vector = self.create_sparse_vector_as_dense(query)\n",
    "        \n",
    "        # Get candidates using both vectors for better coverage\n",
    "        candidate_limit = limit * 3\n",
    "        \n",
    "        # Retrieve dense candidates\n",
    "        dense_results = self.client.search(\n",
    "            collection_name=self.collection_name,\n",
    "            query_vector=(\"dense\", dense_vector),\n",
    "            limit=candidate_limit\n",
    "        )\n",
    "        \n",
    "        # Retrieve sparse candidates\n",
    "        sparse_results = self.client.search(\n",
    "            collection_name=self.collection_name,\n",
    "            query_vector=(\"sparse\", sparse_vector),\n",
    "            limit=candidate_limit\n",
    "        )\n",
    "        \n",
    "        # Combine candidate IDs\n",
    "        dense_scores = {hit.id: hit.score for hit in dense_results}\n",
    "        sparse_scores = {hit.id: hit.score for hit in sparse_results}\n",
    "        all_ids = set(dense_scores.keys()) | set(sparse_scores.keys())\n",
    "        \n",
    "        # Score candidates using weighted formula\n",
    "        combined_scores = []\n",
    "        for doc_id in all_ids:\n",
    "            dense_score = dense_scores.get(doc_id, 0.0)\n",
    "            sparse_score = sparse_scores.get(doc_id, 0.0)\n",
    "            \n",
    "            # Weighted combination\n",
    "            combined_score = (dense_weight * dense_score) + (sparse_weight * sparse_score)\n",
    "            \n",
    "            combined_scores.append((doc_id, combined_score))\n",
    "        \n",
    "        # Sort by combined score and get top results\n",
    "        combined_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_ids = [doc_id for doc_id, _ in combined_scores[:limit]]\n",
    "        \n",
    "        if not top_ids:\n",
    "            return []\n",
    "        \n",
    "        # Fetch full results for top IDs\n",
    "        points = self.client.retrieve(\n",
    "            collection_name=self.collection_name,\n",
    "            ids=top_ids\n",
    "        )\n",
    "        \n",
    "        # Format results\n",
    "        results = []\n",
    "        for point in points:\n",
    "            score = next((score for id, score in combined_scores if id == point.id), 0.0)\n",
    "            results.append({\n",
    "                'text': point.payload.get('text', ''),\n",
    "                'id': point.payload.get('id', point.id),\n",
    "                'score': score\n",
    "            })\n",
    "        \n",
    "        # Sort by score\n",
    "        results.sort(key=lambda x: x['score'], reverse=True)\n",
    "        \n",
    "        return results\n",
    "\n",
    "class MetricsCalculator:\n",
    "    \"\"\"Class for calculating search quality metrics.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_recall_at_k(relevant_ids: List[int], retrieved_ids: List[int], k: int) -> float:\n",
    "        \"\"\"Calculate Recall@k for a single query.\"\"\"\n",
    "        if not relevant_ids:\n",
    "            return 0.0\n",
    "        \n",
    "        relevant_retrieved = set(relevant_ids).intersection(set(retrieved_ids[:k]))\n",
    "        return len(relevant_retrieved) / len(relevant_ids)\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_mrr_at_k(relevant_ids: List[int], retrieved_ids: List[int], k: int) -> float:\n",
    "        \"\"\"Calculate MRR@k (Mean Reciprocal Rank) for a single query.\"\"\"\n",
    "        if not relevant_ids or not retrieved_ids:\n",
    "            return 0.0\n",
    "        \n",
    "        for i, doc_id in enumerate(retrieved_ids[:k]):\n",
    "            if doc_id in relevant_ids:\n",
    "                return 1.0 / (i + 1)\n",
    "        return 0.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_average_metrics(metrics_list: List[Dict[str, float]]) -> Dict[str, float]:\n",
    "        \"\"\"Calculate average metrics across all queries.\"\"\"\n",
    "        if not metrics_list:\n",
    "            return {}\n",
    "        \n",
    "        result = {}\n",
    "        all_keys = set()\n",
    "        for metrics in metrics_list:\n",
    "            all_keys.update(metrics.keys())\n",
    "        \n",
    "        for key in all_keys:\n",
    "            values = [metrics.get(key, 0.0) for metrics in metrics_list]\n",
    "            result[key] = sum(values) / len(metrics_list)\n",
    "        \n",
    "        return result\n",
    "\n",
    "class HybridSearchEvaluator:\n",
    "    \"\"\"Evaluates hybrid search performance.\"\"\"\n",
    "    \n",
    "    def __init__(self, data_path: str = 'qdrant_documentation_dataset.csv'):\n",
    "        self.hybrid_search = HybridSearchManager()\n",
    "        self.metrics_calculator = MetricsCalculator()\n",
    "        self.df = None\n",
    "        self.section_id_map = {}\n",
    "        self.is_initialized = False\n",
    "        self.load_data(data_path)\n",
    "    \n",
    "    def load_data(self, file_path: str):\n",
    "        \"\"\"Load data from CSV file.\"\"\"\n",
    "        self.df = pd.read_csv(file_path)\n",
    "        print(f\"Loaded {len(self.df)} records from dataset.\")\n",
    "    \n",
    "    def initialize_database(self):\n",
    "        \"\"\"Initialize the database with documents from the dataset.\"\"\"\n",
    "        if self.is_initialized:\n",
    "            return\n",
    "        \n",
    "        if self.df is None:\n",
    "            raise ValueError(\"Data not loaded. Call load_data() first.\")\n",
    "        \n",
    "        # Get unique chunks of content\n",
    "        sections = self.df['section_content'].unique()\n",
    "        print(f\"Found {len(sections)} unique chunks for indexing.\")\n",
    "        \n",
    "        # Create mapping of section_content -> id\n",
    "        for idx, section in enumerate(sections):\n",
    "            self.section_id_map[section] = idx\n",
    "        \n",
    "        # Initialize Qdrant collection\n",
    "        self.hybrid_search.initialize_collection()\n",
    "        \n",
    "        # Compute embeddings and add to Qdrant\n",
    "        batch_size = 50  # Smaller batch size to avoid memory issues\n",
    "        for i in range(0, len(sections), batch_size):\n",
    "            batch = sections[i:i + batch_size]\n",
    "            \n",
    "            # Prepare payloads\n",
    "            payloads = [\n",
    "                {\n",
    "                    'text': section,\n",
    "                    'id': self.section_id_map[section]\n",
    "                }\n",
    "                for section in batch\n",
    "            ]\n",
    "            \n",
    "            # Add to Qdrant\n",
    "            print(f\"Processing batch {i//batch_size + 1}/{(len(sections) - 1)//batch_size + 1}\")\n",
    "            self.hybrid_search.upsert_batch(i, batch, payloads)\n",
    "        \n",
    "        self.is_initialized = True\n",
    "        print(\"Database successfully initialized.\")\n",
    "    \n",
    "    def evaluate_hybrid_search(self, hybrid_methods: List[str] = [\"fusion\", \"dense_rerank\", \"sparse_rerank\"],\n",
    "                              k_values: List[int] = [1, 4, 6]) -> Dict[str, Dict[str, float]]:\n",
    "        \"\"\"Evaluate hybrid search with different methods and metrics.\"\"\"\n",
    "        if not self.is_initialized:\n",
    "            self.initialize_database()\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for method in hybrid_methods:\n",
    "            print(f\"\\n===== Evaluating hybrid method: {method} =====\")\n",
    "            \n",
    "            all_metrics = []\n",
    "            \n",
    "            # Process each question in the dataset\n",
    "            for idx, row in tqdm(self.df.iterrows(), total=len(self.df), \n",
    "                               desc=f\"Evaluating {method}\"):\n",
    "                question = row['question']\n",
    "                relevant_section = row['section_content']\n",
    "                relevant_id = self.section_id_map[relevant_section]\n",
    "                \n",
    "                # Get search results\n",
    "                search_results = self.hybrid_search.hybrid_search(\n",
    "                    question, \n",
    "                    limit=max(k_values),\n",
    "                    hybrid_method=method\n",
    "                )\n",
    "                \n",
    "                # Check if we got valid results\n",
    "                if not search_results:\n",
    "                    # Empty results, count as 0 for all metrics\n",
    "                    query_metrics = {f'Recall@{k}': 0.0 for k in k_values}\n",
    "                    query_metrics.update({f'MRR@{k}': 0.0 for k in k_values})\n",
    "                else:\n",
    "                    # Extract IDs from results\n",
    "                    retrieved_ids = [result['id'] for result in search_results]\n",
    "                    \n",
    "                    # Calculate metrics for current query\n",
    "                    query_metrics = {}\n",
    "                    for k in k_values:\n",
    "                        query_metrics[f'Recall@{k}'] = self.metrics_calculator.calculate_recall_at_k(\n",
    "                            [relevant_id], retrieved_ids, k)\n",
    "                        query_metrics[f'MRR@{k}'] = self.metrics_calculator.calculate_mrr_at_k(\n",
    "                            [relevant_id], retrieved_ids, k)\n",
    "                \n",
    "                all_metrics.append(query_metrics)\n",
    "            \n",
    "            # Compute average metrics across all queries\n",
    "            average_metrics = self.metrics_calculator.compute_average_metrics(all_metrics)\n",
    "            \n",
    "            # Display results\n",
    "            for metric_name, value in sorted(average_metrics.items()):\n",
    "                print(f\"{metric_name}: {value:.4f}\")\n",
    "            \n",
    "            # Store results for this method\n",
    "            results[method] = average_metrics\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def compare_with_baseline(self, results: Dict[str, Dict[str, float]]):\n",
    "        \"\"\"Compare hybrid search methods with baseline.\"\"\"\n",
    "        # Baseline metrics from earlier experiments\n",
    "        baseline = {\n",
    "            'MRR@1': 0.6250,\n",
    "            'MRR@4': 0.7790,\n",
    "            'MRR@6': 0.7836,\n",
    "            'Recall@1': 0.7104,\n",
    "            'Recall@4': 0.8780,\n",
    "            'Recall@6': 0.9024\n",
    "        }\n",
    "        \n",
    "        # Add baseline to results\n",
    "        results['Best from previous experiments'] = baseline\n",
    "        \n",
    "        # Print comparison table\n",
    "        print(\"\\n===== Comparison with baseline =====\")\n",
    "        \n",
    "        # Get all metrics from results\n",
    "        metrics = set()\n",
    "        for method_metrics in results.values():\n",
    "            metrics.update(method_metrics.keys())\n",
    "        \n",
    "        # For consistent ordering\n",
    "        metrics = sorted(metrics)\n",
    "        methods = sorted([m for m in results.keys() if m != 'Best from previous experiments']) + ['Best from previous experiments']\n",
    "        \n",
    "        # Print header\n",
    "        header = \"Metric     | \" + \" | \".join(f\"{method:<16}\" for method in methods)\n",
    "        print(header)\n",
    "        print(\"-\" * len(header))\n",
    "        \n",
    "        # Prioritize MRR and Recall metrics\n",
    "        metrics_order = []\n",
    "        for prefix in ['MRR@', 'Recall@']:\n",
    "            for metric in metrics:\n",
    "                if metric.startswith(prefix):\n",
    "                    metrics_order.append(metric)\n",
    "        \n",
    "        for metric in metrics_order:\n",
    "            baseline_value = baseline.get(metric, 0.0)\n",
    "            row = f\"{metric:<10} | \"\n",
    "            \n",
    "            for method in methods:\n",
    "                value = results[method].get(metric, 0.0)\n",
    "                \n",
    "                # Calculate percentage difference from baseline\n",
    "                if baseline_value > 0:\n",
    "                    pct_diff = ((value - baseline_value) / baseline_value) * 100\n",
    "                    if method == 'Best from previous experiments':\n",
    "                        direction = \"↓\"  # Just a neutral indicator for baseline\n",
    "                        pct_diff = 0.0  # No difference from itself\n",
    "                    else:\n",
    "                        direction = \"↑\" if pct_diff > 0 else \"↓\"\n",
    "                    \n",
    "                    row += f\"{value:.4f} {direction} {abs(pct_diff):.1f}% | \"\n",
    "                else:\n",
    "                    row += f\"{value:.4f} | \"\n",
    "            \n",
    "            print(row.rstrip(\" | \"))\n",
    "        \n",
    "        # Find best method for each metric\n",
    "        print(\"\\n===== Best Methods =====\")\n",
    "        for metric in metrics_order:\n",
    "            best_method = None\n",
    "            best_value = -1\n",
    "            for method in results:\n",
    "                value = results[method].get(metric, 0.0)\n",
    "                if value > best_value:\n",
    "                    best_value = value\n",
    "                    best_method = method\n",
    "            \n",
    "            # Calculate improvement over baseline\n",
    "            baseline_value = baseline.get(metric, 0.0)\n",
    "            if baseline_value > 0:\n",
    "                pct_diff = ((best_value - baseline_value) / baseline_value) * 100\n",
    "                print(f\"{metric}: {best_method} ({best_value:.4f}, {'+' if pct_diff > 0 else ''}{pct_diff:.1f}%)\")\n",
    "            else:\n",
    "                print(f\"{metric}: {best_method} ({best_value:.4f})\")\n",
    "    \n",
    "    def find_optimal_weights(self, k_values: List[int] = [1, 4, 6], \n",
    "                          weight_steps: int = 5) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Perform grid search to find optimal weights for hybrid search.\n",
    "        \n",
    "        Args:\n",
    "            k_values: List of k values for evaluation metrics\n",
    "            weight_steps: Number of steps for grid search (from 0 to 1)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with grid search results and best configuration\n",
    "        \"\"\"\n",
    "        if not self.is_initialized:\n",
    "            self.initialize_database()\n",
    "        \n",
    "        print(\"\\n===== Finding optimal weights for hybrid search =====\")\n",
    "        \n",
    "        # Generate weight combinations\n",
    "        weights = np.linspace(0, 1, weight_steps + 1)\n",
    "        \n",
    "        results = {}\n",
    "        best_config = {\n",
    "            \"weights\": None,\n",
    "            \"metrics\": None,\n",
    "            \"best_mrr\": 0.0\n",
    "        }\n",
    "        \n",
    "        # For tracking progress\n",
    "        total_configs = len(weights) * len(weights)\n",
    "        config_count = 0\n",
    "        \n",
    "        for dense_weight in weights:\n",
    "            for sparse_weight in weights:\n",
    "                # Skip if both weights are 0\n",
    "                if dense_weight == 0 and sparse_weight == 0:\n",
    "                    continue\n",
    "                \n",
    "                config_count += 1\n",
    "                print(f\"\\nTesting weights {config_count}/{total_configs}: \"\n",
    "                      f\"Dense={dense_weight:.2f}, Sparse={sparse_weight:.2f}\")\n",
    "                \n",
    "                weight_config = (float(dense_weight), float(sparse_weight))  # Convert to float explicitly\n",
    "                all_metrics = []\n",
    "                \n",
    "                # Process a sample of questions for efficiency\n",
    "                sample_size = min(50, len(self.df))\n",
    "                sample_df = self.df.sample(sample_size, random_state=42)  # Fixed random state for reproducibility\n",
    "                \n",
    "                for idx, row in tqdm(sample_df.iterrows(), total=len(sample_df), \n",
    "                                   desc=\"Evaluating weights\"):\n",
    "                    question = row['question']\n",
    "                    relevant_section = row['section_content']\n",
    "                    relevant_id = self.section_id_map[relevant_section]\n",
    "                    \n",
    "                    # Get search results with current weights\n",
    "                    search_results = self.hybrid_search.weighted_hybrid_search(\n",
    "                        question,\n",
    "                        dense_weight=dense_weight,\n",
    "                        sparse_weight=sparse_weight,\n",
    "                        limit=max(k_values)\n",
    "                    )\n",
    "                    \n",
    "                    # Check if we got valid results\n",
    "                    if not search_results:\n",
    "                        # Empty results, count as 0 for all metrics\n",
    "                        query_metrics = {f'Recall@{k}': 0.0 for k in k_values}\n",
    "                        query_metrics.update({f'MRR@{k}': 0.0 for k in k_values})\n",
    "                    else:\n",
    "                        # Calculate metrics\n",
    "                        retrieved_ids = [result['id'] for result in search_results]\n",
    "                        \n",
    "                        # Calculate metrics for current query\n",
    "                        query_metrics = {}\n",
    "                        for k in k_values:\n",
    "                            query_metrics[f'Recall@{k}'] = self.metrics_calculator.calculate_recall_at_k(\n",
    "                                [relevant_id], retrieved_ids, k)\n",
    "                            query_metrics[f'MRR@{k}'] = self.metrics_calculator.calculate_mrr_at_k(\n",
    "                                [relevant_id], retrieved_ids, k)\n",
    "                    \n",
    "                    all_metrics.append(query_metrics)\n",
    "                \n",
    "                # Calculate average metrics\n",
    "                average_metrics = self.metrics_calculator.compute_average_metrics(all_metrics)\n",
    "                results[weight_config] = average_metrics\n",
    "                \n",
    "                # Print current results\n",
    "                for metric_name, value in sorted(average_metrics.items()):\n",
    "                    print(f\"{metric_name}: {value:.4f}\")\n",
    "                \n",
    "                # Update best configuration based on MRR@4\n",
    "                mrr4 = average_metrics.get('MRR@4', 0.0)\n",
    "                if mrr4 > best_config['best_mrr']:\n",
    "                    best_config['best_mrr'] = mrr4\n",
    "                    best_config['weights'] = weight_config\n",
    "                    best_config['metrics'] = average_metrics\n",
    "        \n",
    "        # Print the best configuration\n",
    "        print(\"\\n===== Best Weight Configuration =====\")\n",
    "        print(f\"Dense Weight: {best_config['weights'][0]:.2f}\")\n",
    "        print(f\"Sparse Weight: {best_config['weights'][1]:.2f}\")\n",
    "        print(\"Metrics:\")\n",
    "        for metric_name, value in sorted(best_config['metrics'].items()):\n",
    "            print(f\"{metric_name}: {value:.4f}\")\n",
    "        \n",
    "        return {\"results\": results, \"best\": best_config}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fdb3fd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_hybrid_search(self, query: str, dense_weight: float = 0.5, \n",
    "                          sparse_weight: float = 0.5, limit: int = 6) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Perform weighted hybrid search using a custom scoring function.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query\n",
    "        dense_weight: Weight for dense vector scores (0.0 to 1.0)\n",
    "        sparse_weight: Weight for sparse vector scores (0.0 to 1.0)\n",
    "        limit: Number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        List of search results with custom-weighted scores\n",
    "    \"\"\"\n",
    "    if not (0 <= dense_weight <= 1) or not (0 <= sparse_weight <= 1):\n",
    "        raise ValueError(\"Weights must be between 0 and 1\")\n",
    "    \n",
    "    # Normalize weights to sum to 1.0\n",
    "    total_weight = dense_weight + sparse_weight\n",
    "    if total_weight == 0:\n",
    "        raise ValueError(\"At least one weight must be positive\")\n",
    "    \n",
    "    dense_weight = dense_weight / total_weight\n",
    "    sparse_weight = sparse_weight / total_weight\n",
    "    \n",
    "    # Encode query\n",
    "    dense_vector = self.dense_model.encode(query, normalize_embeddings=True).tolist()\n",
    "    sparse_vector = self.create_sparse_vector_as_dense(query)\n",
    "    \n",
    "    # Get candidates using both vectors for better coverage\n",
    "    candidate_limit = limit * 3\n",
    "    \n",
    "    # Retrieve dense candidates\n",
    "    try:\n",
    "        dense_results = self.client.search(\n",
    "            collection_name=self.collection_name,\n",
    "            query_vector=(\"dense\", dense_vector),\n",
    "            limit=candidate_limit\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error in dense search: {e}\")\n",
    "        dense_results = []\n",
    "    \n",
    "    # Retrieve sparse candidates\n",
    "    try:\n",
    "        sparse_results = self.client.search(\n",
    "            collection_name=self.collection_name,\n",
    "            query_vector=(\"sparse\", sparse_vector),\n",
    "            limit=candidate_limit\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error in sparse search: {e}\")\n",
    "        sparse_results = []\n",
    "    \n",
    "    # Combine candidate IDs\n",
    "    dense_scores = {hit.id: hit.score for hit in dense_results}\n",
    "    sparse_scores = {hit.id: hit.score for hit in sparse_results}\n",
    "    all_ids = set(dense_scores.keys()) | set(sparse_scores.keys())\n",
    "    \n",
    "    if not all_ids:\n",
    "        return []\n",
    "    \n",
    "    # Score candidates using weighted formula\n",
    "    combined_scores = []\n",
    "    for doc_id in all_ids:\n",
    "        dense_score = dense_scores.get(doc_id, 0.0)\n",
    "        sparse_score = sparse_scores.get(doc_id, 0.0)\n",
    "        \n",
    "        # Weighted combination\n",
    "        combined_score = (dense_weight * dense_score) + (sparse_weight * sparse_score)\n",
    "        \n",
    "        combined_scores.append((doc_id, combined_score))\n",
    "    \n",
    "    # Sort by combined score and get top results\n",
    "    combined_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_ids = [doc_id for doc_id, _ in combined_scores[:limit]]\n",
    "    \n",
    "    if not top_ids:\n",
    "        return []\n",
    "    \n",
    "    # Fetch full results for top IDs\n",
    "    try:\n",
    "        points = self.client.retrieve(\n",
    "            collection_name=self.collection_name,\n",
    "            ids=top_ids\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving points: {e}\")\n",
    "        return []\n",
    "    \n",
    "    # Format results\n",
    "    results = []\n",
    "    for point in points:\n",
    "        score = next((score for id, score in combined_scores if id == point.id), 0.0)\n",
    "        results.append({\n",
    "            'text': point.payload.get('text', ''),\n",
    "            'id': point.payload.get('id', point.id),\n",
    "            'score': score\n",
    "        })\n",
    "    \n",
    "    # Sort by score\n",
    "    results.sort(key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "be7cc001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_hybrid_search(self, query: str, dense_weight: float = 0.5, \n",
    "                          sparse_weight: float = 0.5, limit: int = 6) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Perform weighted hybrid search using a custom scoring function.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query\n",
    "        dense_weight: Weight for dense vector scores (0.0 to 1.0)\n",
    "        sparse_weight: Weight for sparse vector scores (0.0 to 1.0)\n",
    "        limit: Number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        List of search results with custom-weighted scores\n",
    "    \"\"\"\n",
    "    if not (0 <= dense_weight <= 1) or not (0 <= sparse_weight <= 1):\n",
    "        raise ValueError(\"Weights must be between 0 and 1\")\n",
    "    \n",
    "    # Normalize weights to sum to 1.0\n",
    "    total_weight = dense_weight + sparse_weight\n",
    "    if total_weight == 0:\n",
    "        raise ValueError(\"At least one weight must be positive\")\n",
    "    \n",
    "    dense_weight = dense_weight / total_weight\n",
    "    sparse_weight = sparse_weight / total_weight\n",
    "    \n",
    "    # Encode query\n",
    "    dense_vector = self.dense_model.encode(query, normalize_embeddings=True).tolist()\n",
    "    sparse_vector = self.create_sparse_vector(query)\n",
    "    \n",
    "    # Get candidates using both vectors for better coverage\n",
    "    candidate_limit = limit * 3\n",
    "    \n",
    "    # Retrieve dense candidates\n",
    "    dense_results = self.client.search(\n",
    "        collection_name=self.collection_name,\n",
    "        query_vector=(\"dense\", dense_vector),\n",
    "        limit=candidate_limit\n",
    "    )\n",
    "    \n",
    "    # Retrieve sparse candidates\n",
    "    sparse_results = self.client.search(\n",
    "        collection_name=self.collection_name,\n",
    "        query_vector=(\"sparse\", sparse_vector),\n",
    "        limit=candidate_limit\n",
    "    )\n",
    "    \n",
    "    # Combine candidate IDs\n",
    "    dense_ids = {hit.id: hit.score for hit in dense_results}\n",
    "    sparse_ids = {hit.id: hit.score for hit in sparse_results}\n",
    "    all_ids = set(dense_ids.keys()) | set(sparse_ids.keys())\n",
    "    \n",
    "    # Score candidates using weighted formula\n",
    "    combined_scores = []\n",
    "    for doc_id in all_ids:\n",
    "        dense_score = dense_ids.get(doc_id, 0.0)\n",
    "        sparse_score = sparse_ids.get(doc_id, 0.0)\n",
    "        \n",
    "        # Weighted combination\n",
    "        combined_score = (dense_weight * dense_score) + (sparse_weight * sparse_score)\n",
    "        \n",
    "        combined_scores.append((doc_id, combined_score))\n",
    "    \n",
    "    # Sort by combined score and get top results\n",
    "    combined_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_ids = combined_scores[:limit]\n",
    "    \n",
    "    # Fetch full results for top IDs\n",
    "    results = []\n",
    "    for doc_id, score in top_ids:\n",
    "        point = self.client.retrieve(\n",
    "            collection_name=self.collection_name,\n",
    "            ids=[doc_id]\n",
    "        )[0]\n",
    "        \n",
    "        results.append({\n",
    "            'text': point.payload.get('text', ''),\n",
    "            'id': point.payload.get('id', point.id),\n",
    "            'score': score\n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ff3d359f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_weights(self, k_values: List[int] = [1, 4, 6], \n",
    "                        weight_steps: int = 5) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Perform grid search to find optimal weights for hybrid search.\n",
    "    \n",
    "    Args:\n",
    "        k_values: List of k values for evaluation metrics\n",
    "        weight_steps: Number of steps for grid search (from 0 to 1)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with grid search results and best configuration\n",
    "    \"\"\"\n",
    "    if not self.is_initialized:\n",
    "        self.initialize_database()\n",
    "    \n",
    "    print(\"\\n===== Finding optimal weights for hybrid search =====\")\n",
    "    \n",
    "    # Generate weight combinations\n",
    "    weights = np.linspace(0, 1, weight_steps + 1)\n",
    "    \n",
    "    results = {}\n",
    "    best_config = {\n",
    "        \"weights\": None,\n",
    "        \"metrics\": None,\n",
    "        \"best_mrr\": 0.0\n",
    "    }\n",
    "    \n",
    "    # For tracking progress\n",
    "    total_configs = len(weights) * len(weights)\n",
    "    config_count = 0\n",
    "    \n",
    "    for dense_weight in weights:\n",
    "        for sparse_weight in weights:\n",
    "            # Skip if both weights are 0\n",
    "            if dense_weight == 0 and sparse_weight == 0:\n",
    "                continue\n",
    "            \n",
    "            config_count += 1\n",
    "            print(f\"\\nTesting weights {config_count}/{total_configs}: \"\n",
    "                  f\"Dense={dense_weight:.2f}, Sparse={sparse_weight:.2f}\")\n",
    "            \n",
    "            weight_config = (dense_weight, sparse_weight)\n",
    "            all_metrics = []\n",
    "            \n",
    "            # Process a sample of questions for efficiency\n",
    "            sample_size = min(50, len(self.df))\n",
    "            sample_df = self.df.sample(sample_size)\n",
    "            \n",
    "            for idx, row in tqdm(sample_df.iterrows(), total=len(sample_df), \n",
    "                               desc=\"Evaluating weights\"):\n",
    "                question = row['question']\n",
    "                relevant_section = row['section_content']\n",
    "                relevant_id = self.section_id_map[relevant_section]\n",
    "                \n",
    "                # Get search results with current weights\n",
    "                search_results = self.hybrid_search.weighted_hybrid_search(\n",
    "                    question,\n",
    "                    dense_weight=dense_weight,\n",
    "                    sparse_weight=sparse_weight,\n",
    "                    limit=max(k_values)\n",
    "                )\n",
    "                retrieved_ids = [result['id'] for result in search_results]\n",
    "                \n",
    "                # Calculate metrics for current query\n",
    "                query_metrics = {}\n",
    "                for k in k_values:\n",
    "                    query_metrics[f'Recall@{k}'] = self.metrics_calculator.calculate_recall_at_k(\n",
    "                        [relevant_id], retrieved_ids, k)\n",
    "                    query_metrics[f'MRR@{k}'] = self.metrics_calculator.calculate_mrr_at_k(\n",
    "                        [relevant_id], retrieved_ids, k)\n",
    "                \n",
    "                all_metrics.append(query_metrics)\n",
    "            \n",
    "            # Calculate average metrics\n",
    "            average_metrics = self.metrics_calculator.compute_average_metrics(all_metrics)\n",
    "            results[weight_config] = average_metrics\n",
    "            \n",
    "            # Print current results\n",
    "            for metric_name, value in sorted(average_metrics.items()):\n",
    "                print(f\"{metric_name}: {value:.4f}\")\n",
    "            \n",
    "            # Update best configuration based on MRR@4\n",
    "            mrr4 = average_metrics.get('MRR@4', 0.0)\n",
    "            if mrr4 > best_config['best_mrr']:\n",
    "                best_config['best_mrr'] = mrr4\n",
    "                best_config['weights'] = weight_config\n",
    "                best_config['metrics'] = average_metrics\n",
    "    \n",
    "    # Print the best configuration\n",
    "    print(\"\\n===== Best Weight Configuration =====\")\n",
    "    print(f\"Dense Weight: {best_config['weights'][0]:.2f}\")\n",
    "    print(f\"Sparse Weight: {best_config['weights'][1]:.2f}\")\n",
    "    print(\"Metrics:\")\n",
    "    for metric_name, value in sorted(best_config['metrics'].items()):\n",
    "        print(f\"{metric_name}: {value:.4f}\")\n",
    "    \n",
    "    return {\"results\": results, \"best\": best_config}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8d4ad4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_weights(self, k_values: List[int] = [1, 4, 6], \n",
    "                        weight_steps: int = 5) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Perform grid search to find optimal weights for hybrid search.\n",
    "    \n",
    "    Args:\n",
    "        k_values: List of k values for evaluation metrics\n",
    "        weight_steps: Number of steps for grid search (from 0 to 1)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with grid search results and best configuration\n",
    "    \"\"\"\n",
    "    if not self.is_initialized:\n",
    "        self.initialize_database()\n",
    "    \n",
    "    print(\"\\n===== Finding optimal weights for hybrid search =====\")\n",
    "    \n",
    "    # Generate weight combinations\n",
    "    weights = np.linspace(0, 1, weight_steps + 1)\n",
    "    \n",
    "    results = {}\n",
    "    best_config = {\n",
    "        \"weights\": None,\n",
    "        \"metrics\": None,\n",
    "        \"best_mrr\": 0.0\n",
    "    }\n",
    "    \n",
    "    # For tracking progress\n",
    "    total_configs = len(weights) * len(weights)\n",
    "    config_count = 0\n",
    "    \n",
    "    for dense_weight in weights:\n",
    "        for sparse_weight in weights:\n",
    "            # Skip if both weights are 0\n",
    "            if dense_weight == 0 and sparse_weight == 0:\n",
    "                continue\n",
    "            \n",
    "            config_count += 1\n",
    "            print(f\"\\nTesting weights {config_count}/{total_configs}: \"\n",
    "                  f\"Dense={dense_weight:.2f}, Sparse={sparse_weight:.2f}\")\n",
    "            \n",
    "            weight_config = (float(dense_weight), float(sparse_weight))  # Convert to float explicitly\n",
    "            all_metrics = []\n",
    "            \n",
    "            # Process a sample of questions for efficiency\n",
    "            sample_size = min(50, len(self.df))\n",
    "            sample_df = self.df.sample(sample_size, random_state=42)  # Fixed random state for reproducibility\n",
    "            \n",
    "            for idx, row in tqdm(sample_df.iterrows(), total=len(sample_df), \n",
    "                               desc=\"Evaluating weights\"):\n",
    "                question = row['question']\n",
    "                relevant_section = row['section_content']\n",
    "                relevant_id = self.section_id_map[relevant_section]\n",
    "                \n",
    "                # Get search results with current weights\n",
    "                search_results = self.hybrid_search.weighted_hybrid_search(\n",
    "                    question,\n",
    "                    dense_weight=dense_weight,\n",
    "                    sparse_weight=sparse_weight,\n",
    "                    limit=max(k_values)\n",
    "                )\n",
    "                \n",
    "                # Check if we got valid results\n",
    "                if not search_results:\n",
    "                    # Empty results, count as 0 for all metrics\n",
    "                    query_metrics = {f'Recall@{k}': 0.0 for k in k_values}\n",
    "                    query_metrics.update({f'MRR@{k}': 0.0 for k in k_values})\n",
    "                else:\n",
    "                    # Calculate metrics\n",
    "                    retrieved_ids = [result['id'] for result in search_results]\n",
    "                    \n",
    "                    # Calculate metrics for current query\n",
    "                    query_metrics = {}\n",
    "                    for k in k_values:\n",
    "                        query_metrics[f'Recall@{k}'] = self.metrics_calculator.calculate_recall_at_k(\n",
    "                            [relevant_id], retrieved_ids, k)\n",
    "                        query_metrics[f'MRR@{k}'] = self.metrics_calculator.calculate_mrr_at_k(\n",
    "                            [relevant_id], retrieved_ids, k)\n",
    "                \n",
    "                all_metrics.append(query_metrics)\n",
    "            \n",
    "            # Calculate average metrics\n",
    "            average_metrics = self.metrics_calculator.compute_average_metrics(all_metrics)\n",
    "            results[weight_config] = average_metrics\n",
    "            \n",
    "            # Print current results\n",
    "            for metric_name, value in sorted(average_metrics.items()):\n",
    "                print(f\"{metric_name}: {value:.4f}\")\n",
    "            \n",
    "            # Update best configuration based on MRR@4\n",
    "            mrr4 = average_metrics.get('MRR@4', 0.0)\n",
    "            if mrr4 > best_config['best_mrr']:\n",
    "                best_config['best_mrr'] = mrr4\n",
    "                best_config['weights'] = weight_config\n",
    "                best_config['metrics'] = average_metrics\n",
    "    \n",
    "    # Print the best configuration\n",
    "    print(\"\\n===== Best Weight Configuration =====\")\n",
    "    print(f\"Dense Weight: {best_config['weights'][0]:.2f}\")\n",
    "    print(f\"Sparse Weight: {best_config['weights'][1]:.2f}\")\n",
    "    print(\"Metrics:\")\n",
    "    for metric_name, value in sorted(best_config['metrics'].items()):\n",
    "        print(f\"{metric_name}: {value:.4f}\")\n",
    "    \n",
    "    return {\"results\": results, \"best\": best_config}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "776bc74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Hybrid Search Evaluation =====\n",
      "Loaded 328 records from dataset.\n",
      "Found 121 unique chunks for indexing.\n",
      "Processing batch 1/3\n",
      "Processing batch 2/3\n",
      "Processing batch 3/3\n",
      "Database successfully initialized.\n",
      "\n",
      "===== Evaluating hybrid method: fusion =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating fusion:  87%|████████▋ | 284/328 [01:20<00:03, 11.17it/s]C:\\Users\\sekho\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\qdrant_client\\local\\distances.py:76: RuntimeWarning: invalid value encountered in divide\n",
      "  query = query / np.linalg.norm(query)\n",
      "Evaluating fusion: 100%|██████████| 328/328 [01:24<00:00,  3.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1: 0.6463\n",
      "MRR@4: 0.7508\n",
      "MRR@6: 0.7581\n",
      "Recall@1: 0.6463\n",
      "Recall@4: 0.8811\n",
      "Recall@6: 0.9207\n",
      "\n",
      "===== Evaluating hybrid method: dense_rerank =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating dense_rerank: 100%|██████████| 328/328 [00:35<00:00,  9.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1: 0.0000\n",
      "MRR@4: 0.0000\n",
      "MRR@6: 0.0000\n",
      "Recall@1: 0.0000\n",
      "Recall@4: 0.0000\n",
      "Recall@6: 0.0000\n",
      "\n",
      "===== Evaluating hybrid method: sparse_rerank =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating sparse_rerank: 100%|██████████| 328/328 [00:33<00:00,  9.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1: 0.0000\n",
      "MRR@4: 0.0000\n",
      "MRR@6: 0.0000\n",
      "Recall@1: 0.0000\n",
      "Recall@4: 0.0000\n",
      "Recall@6: 0.0000\n",
      "\n",
      "===== Comparison with baseline =====\n",
      "Metric     | dense_rerank     | fusion           | sparse_rerank    | Best from previous experiments\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MRR@1      | 0.0000 ↓ 100.0% | 0.6463 ↑ 3.4% | 0.0000 ↓ 100.0% | 0.6250 ↓ 0.0%\n",
      "MRR@4      | 0.0000 ↓ 100.0% | 0.7508 ↓ 3.6% | 0.0000 ↓ 100.0% | 0.7790 ↓ 0.0%\n",
      "MRR@6      | 0.0000 ↓ 100.0% | 0.7581 ↓ 3.3% | 0.0000 ↓ 100.0% | 0.7836 ↓ 0.0%\n",
      "Recall@1   | 0.0000 ↓ 100.0% | 0.6463 ↓ 9.0% | 0.0000 ↓ 100.0% | 0.7104 ↓ 0.0%\n",
      "Recall@4   | 0.0000 ↓ 100.0% | 0.8811 ↑ 0.4% | 0.0000 ↓ 100.0% | 0.8780 ↓ 0.0%\n",
      "Recall@6   | 0.0000 ↓ 100.0% | 0.9207 ↑ 2.0% | 0.0000 ↓ 100.0% | 0.9024 ↓ 0.0%\n",
      "\n",
      "===== Best Methods =====\n",
      "MRR@1: fusion (0.6463, +3.4%)\n",
      "MRR@4: Best from previous experiments (0.7790, 0.0%)\n",
      "MRR@6: Best from previous experiments (0.7836, 0.0%)\n",
      "Recall@1: Best from previous experiments (0.7104, 0.0%)\n",
      "Recall@4: fusion (0.8811, +0.4%)\n",
      "Recall@6: fusion (0.9207, +2.0%)\n",
      "\n",
      "===== Finding optimal weights for hybrid search =====\n",
      "\n",
      "Testing weights 1/36: Dense=0.00, Sparse=0.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating weights: 100%|██████████| 50/50 [00:05<00:00,  9.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1: 0.5200\n",
      "MRR@4: 0.5950\n",
      "MRR@6: 0.6023\n",
      "Recall@1: 0.5200\n",
      "Recall@4: 0.7200\n",
      "Recall@6: 0.7600\n",
      "\n",
      "Testing weights 2/36: Dense=0.00, Sparse=0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating weights: 100%|██████████| 50/50 [00:05<00:00,  9.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1: 0.5200\n",
      "MRR@4: 0.5950\n",
      "MRR@6: 0.6023\n",
      "Recall@1: 0.5200\n",
      "Recall@4: 0.7200\n",
      "Recall@6: 0.7600\n",
      "\n",
      "Testing weights 3/36: Dense=0.00, Sparse=0.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating weights: 100%|██████████| 50/50 [00:05<00:00,  9.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1: 0.5200\n",
      "MRR@4: 0.5950\n",
      "MRR@6: 0.6023\n",
      "Recall@1: 0.5200\n",
      "Recall@4: 0.7200\n",
      "Recall@6: 0.7600\n",
      "\n",
      "Testing weights 4/36: Dense=0.00, Sparse=0.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating weights: 100%|██████████| 50/50 [00:04<00:00, 10.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1: 0.5200\n",
      "MRR@4: 0.5950\n",
      "MRR@6: 0.6023\n",
      "Recall@1: 0.5200\n",
      "Recall@4: 0.7200\n",
      "Recall@6: 0.7600\n",
      "\n",
      "Testing weights 5/36: Dense=0.00, Sparse=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating weights: 100%|██████████| 50/50 [00:05<00:00,  9.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1: 0.5200\n",
      "MRR@4: 0.5950\n",
      "MRR@6: 0.6023\n",
      "Recall@1: 0.5200\n",
      "Recall@4: 0.7200\n",
      "Recall@6: 0.7600\n",
      "\n",
      "Testing weights 6/36: Dense=0.20, Sparse=0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating weights: 100%|██████████| 50/50 [00:05<00:00,  9.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1: 0.6000\n",
      "MRR@4: 0.6867\n",
      "MRR@6: 0.6907\n",
      "Recall@1: 0.6000\n",
      "Recall@4: 0.8000\n",
      "Recall@6: 0.8200\n",
      "\n",
      "Testing weights 7/36: Dense=0.20, Sparse=0.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating weights: 100%|██████████| 50/50 [00:05<00:00,  9.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1: 0.6400\n",
      "MRR@4: 0.7050\n",
      "MRR@6: 0.7090\n",
      "Recall@1: 0.6400\n",
      "Recall@4: 0.8000\n",
      "Recall@6: 0.8200\n",
      "\n",
      "Testing weights 8/36: Dense=0.20, Sparse=0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating weights: 100%|██████████| 50/50 [00:05<00:00,  9.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1: 0.5800\n",
      "MRR@4: 0.6650\n",
      "MRR@6: 0.6683\n",
      "Recall@1: 0.5800\n",
      "Recall@4: 0.7800\n",
      "Recall@6: 0.8000\n",
      "\n",
      "Testing weights 9/36: Dense=0.20, Sparse=0.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating weights: 100%|██████████| 50/50 [00:05<00:00,  9.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1: 0.5200\n",
      "MRR@4: 0.6350\n",
      "MRR@6: 0.6423\n",
      "Recall@1: 0.5200\n",
      "Recall@4: 0.7800\n",
      "Recall@6: 0.8200\n",
      "\n",
      "Testing weights 10/36: Dense=0.20, Sparse=0.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating weights: 100%|██████████| 50/50 [00:05<00:00,  9.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1: 0.5400\n",
      "MRR@4: 0.6433\n",
      "MRR@6: 0.6473\n",
      "Recall@1: 0.5400\n",
      "Recall@4: 0.7800\n",
      "Recall@6: 0.8000\n",
      "\n",
      "Testing weights 11/36: Dense=0.20, Sparse=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating weights: 100%|██████████| 50/50 [00:05<00:00,  9.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1: 0.5200\n",
      "MRR@4: 0.6317\n",
      "MRR@6: 0.6357\n",
      "Recall@1: 0.5200\n",
      "Recall@4: 0.7800\n",
      "Recall@6: 0.8000\n",
      "\n",
      "Testing weights 12/36: Dense=0.40, Sparse=0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating weights: 100%|██████████| 50/50 [00:04<00:00, 10.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1: 0.6000\n",
      "MRR@4: 0.6867\n",
      "MRR@6: 0.6907\n",
      "Recall@1: 0.6000\n",
      "Recall@4: 0.8000\n",
      "Recall@6: 0.8200\n",
      "\n",
      "Testing weights 13/36: Dense=0.40, Sparse=0.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating weights: 100%|██████████| 50/50 [00:05<00:00,  9.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1: 0.6200\n",
      "MRR@4: 0.7050\n",
      "MRR@6: 0.7090\n",
      "Recall@1: 0.6200\n",
      "Recall@4: 0.8000\n",
      "Recall@6: 0.8200\n",
      "\n",
      "Testing weights 14/36: Dense=0.40, Sparse=0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating weights: 100%|██████████| 50/50 [00:05<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1: 0.6400\n",
      "MRR@4: 0.7050\n",
      "MRR@6: 0.7090\n",
      "Recall@1: 0.6400\n",
      "Recall@4: 0.8000\n",
      "Recall@6: 0.8200\n",
      "\n",
      "Testing weights 15/36: Dense=0.40, Sparse=0.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating weights: 100%|██████████| 50/50 [00:05<00:00,  9.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1: 0.5800\n",
      "MRR@4: 0.6667\n",
      "MRR@6: 0.6733\n",
      "Recall@1: 0.5800\n",
      "Recall@4: 0.7800\n",
      "Recall@6: 0.8200\n",
      "\n",
      "Testing weights 16/36: Dense=0.40, Sparse=0.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating weights: 100%|██████████| 50/50 [00:05<00:00,  9.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1: 0.5800\n",
      "MRR@4: 0.6650\n",
      "MRR@6: 0.6683\n",
      "Recall@1: 0.5800\n",
      "Recall@4: 0.7800\n",
      "Recall@6: 0.8000\n",
      "\n",
      "Testing weights 17/36: Dense=0.40, Sparse=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating weights: 100%|██████████| 50/50 [00:04<00:00, 10.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1: 0.5600\n",
      "MRR@4: 0.6550\n",
      "MRR@6: 0.6623\n",
      "Recall@1: 0.5600\n",
      "Recall@4: 0.7800\n",
      "Recall@6: 0.8200\n",
      "\n",
      "Testing weights 18/36: Dense=0.60, Sparse=0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating weights: 100%|██████████| 50/50 [00:05<00:00,  9.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1: 0.6000\n",
      "MRR@4: 0.6867\n",
      "MRR@6: 0.6907\n",
      "Recall@1: 0.6000\n",
      "Recall@4: 0.8000\n",
      "Recall@6: 0.8200\n",
      "\n",
      "Testing weights 19/36: Dense=0.60, Sparse=0.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating weights: 100%|██████████| 50/50 [00:05<00:00,  9.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1: 0.6600\n",
      "MRR@4: 0.7350\n",
      "MRR@6: 0.7350\n",
      "Recall@1: 0.6600\n",
      "Recall@4: 0.8200\n",
      "Recall@6: 0.8200\n",
      "\n",
      "Testing weights 20/36: Dense=0.60, Sparse=0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating weights: 100%|██████████| 50/50 [00:05<00:00,  9.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1: 0.6000\n",
      "MRR@4: 0.6883\n",
      "MRR@6: 0.6923\n",
      "Recall@1: 0.6000\n",
      "Recall@4: 0.8000\n",
      "Recall@6: 0.8200\n",
      "\n",
      "Testing weights 21/36: Dense=0.60, Sparse=0.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating weights: 100%|██████████| 50/50 [00:05<00:00,  9.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1: 0.6400\n",
      "MRR@4: 0.7050\n",
      "MRR@6: 0.7090\n",
      "Recall@1: 0.6400\n",
      "Recall@4: 0.8000\n",
      "Recall@6: 0.8200\n",
      "\n",
      "Testing weights 22/36: Dense=0.60, Sparse=0.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating weights: 100%|██████████| 50/50 [00:05<00:00,  9.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1: 0.6000\n",
      "MRR@4: 0.6767\n",
      "MRR@6: 0.6840\n",
      "Recall@1: 0.6000\n",
      "Recall@4: 0.7800\n",
      "Recall@6: 0.8200\n",
      "\n",
      "Testing weights 23/36: Dense=0.60, Sparse=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating weights: 100%|██████████| 50/50 [00:04<00:00, 10.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1: 0.5800\n",
      "MRR@4: 0.6667\n",
      "MRR@6: 0.6700\n",
      "Recall@1: 0.5800\n",
      "Recall@4: 0.7800\n",
      "Recall@6: 0.8000\n",
      "\n",
      "Testing weights 24/36: Dense=0.80, Sparse=0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating weights: 100%|██████████| 50/50 [00:05<00:00,  9.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1: 0.6000\n",
      "MRR@4: 0.6867\n",
      "MRR@6: 0.6907\n",
      "Recall@1: 0.6000\n",
      "Recall@4: 0.8000\n",
      "Recall@6: 0.8200\n",
      "\n",
      "Testing weights 25/36: Dense=0.80, Sparse=0.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating weights: 100%|██████████| 50/50 [00:05<00:00,  9.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1: 0.6600\n",
      "MRR@4: 0.7383\n",
      "MRR@6: 0.7417\n",
      "Recall@1: 0.6600\n",
      "Recall@4: 0.8400\n",
      "Recall@6: 0.8600\n",
      "\n",
      "Testing weights 26/36: Dense=0.80, Sparse=0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating weights: 100%|██████████| 50/50 [00:05<00:00,  9.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1: 0.6200\n",
      "MRR@4: 0.7050\n",
      "MRR@6: 0.7090\n",
      "Recall@1: 0.6200\n",
      "Recall@4: 0.8000\n",
      "Recall@6: 0.8200\n",
      "\n",
      "Testing weights 27/36: Dense=0.80, Sparse=0.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating weights: 100%|██████████| 50/50 [00:05<00:00,  9.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1: 0.6000\n",
      "MRR@4: 0.6883\n",
      "MRR@6: 0.6923\n",
      "Recall@1: 0.6000\n",
      "Recall@4: 0.8000\n",
      "Recall@6: 0.8200\n",
      "\n",
      "Testing weights 28/36: Dense=0.80, Sparse=0.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating weights: 100%|██████████| 50/50 [00:05<00:00,  9.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1: 0.6400\n",
      "MRR@4: 0.7050\n",
      "MRR@6: 0.7090\n",
      "Recall@1: 0.6400\n",
      "Recall@4: 0.8000\n",
      "Recall@6: 0.8200\n",
      "\n",
      "Testing weights 29/36: Dense=0.80, Sparse=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating weights: 100%|██████████| 50/50 [00:05<00:00,  9.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1: 0.6200\n",
      "MRR@4: 0.6900\n",
      "MRR@6: 0.6973\n",
      "Recall@1: 0.6200\n",
      "Recall@4: 0.7800\n",
      "Recall@6: 0.8200\n",
      "\n",
      "Testing weights 30/36: Dense=1.00, Sparse=0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating weights: 100%|██████████| 50/50 [00:05<00:00,  9.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1: 0.6000\n",
      "MRR@4: 0.6867\n",
      "MRR@6: 0.6907\n",
      "Recall@1: 0.6000\n",
      "Recall@4: 0.8000\n",
      "Recall@6: 0.8200\n",
      "\n",
      "Testing weights 31/36: Dense=1.00, Sparse=0.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating weights: 100%|██████████| 50/50 [00:05<00:00,  9.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1: 0.6400\n",
      "MRR@4: 0.7250\n",
      "MRR@6: 0.7290\n",
      "Recall@1: 0.6400\n",
      "Recall@4: 0.8400\n",
      "Recall@6: 0.8600\n",
      "\n",
      "Testing weights 32/36: Dense=1.00, Sparse=0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating weights: 100%|██████████| 50/50 [00:05<00:00,  9.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1: 0.6400\n",
      "MRR@4: 0.7250\n",
      "MRR@6: 0.7250\n",
      "Recall@1: 0.6400\n",
      "Recall@4: 0.8200\n",
      "Recall@6: 0.8200\n",
      "\n",
      "Testing weights 33/36: Dense=1.00, Sparse=0.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating weights: 100%|██████████| 50/50 [00:04<00:00, 10.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1: 0.6000\n",
      "MRR@4: 0.6950\n",
      "MRR@6: 0.6990\n",
      "Recall@1: 0.6000\n",
      "Recall@4: 0.8000\n",
      "Recall@6: 0.8200\n",
      "\n",
      "Testing weights 34/36: Dense=1.00, Sparse=0.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating weights: 100%|██████████| 50/50 [00:05<00:00,  9.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1: 0.6000\n",
      "MRR@4: 0.6883\n",
      "MRR@6: 0.6923\n",
      "Recall@1: 0.6000\n",
      "Recall@4: 0.8000\n",
      "Recall@6: 0.8200\n",
      "\n",
      "Testing weights 35/36: Dense=1.00, Sparse=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating weights: 100%|██████████| 50/50 [00:05<00:00,  8.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1: 0.6400\n",
      "MRR@4: 0.7050\n",
      "MRR@6: 0.7090\n",
      "Recall@1: 0.6400\n",
      "Recall@4: 0.8000\n",
      "Recall@6: 0.8200\n",
      "\n",
      "===== Best Weight Configuration =====\n",
      "Dense Weight: 0.80\n",
      "Sparse Weight: 0.20\n",
      "Metrics:\n",
      "MRR@1: 0.6600\n",
      "MRR@4: 0.7383\n",
      "MRR@6: 0.7417\n",
      "Recall@1: 0.6600\n",
      "Recall@4: 0.8400\n",
      "Recall@6: 0.8600\n",
      "\n",
      "===== Evaluating with best weights =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating best weights: 100%|██████████| 328/328 [00:39<00:00,  8.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Comparison with baseline =====\n",
      "Metric     | dense_rerank     | fusion           | sparse_rerank    | weighted_optimal | Best from previous experiments\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "MRR@1      | 0.0000 ↓ 100.0% | 0.6463 ↑ 3.4% | 0.0000 ↓ 100.0% | 0.7134 ↑ 14.1% | 0.6250 ↓ 0.0%\n",
      "MRR@4      | 0.0000 ↓ 100.0% | 0.7508 ↓ 3.6% | 0.0000 ↓ 100.0% | 0.7932 ↑ 1.8% | 0.7790 ↓ 0.0%\n",
      "MRR@6      | 0.0000 ↓ 100.0% | 0.7581 ↓ 3.3% | 0.0000 ↓ 100.0% | 0.8000 ↑ 2.1% | 0.7836 ↓ 0.0%\n",
      "Recall@1   | 0.0000 ↓ 100.0% | 0.6463 ↓ 9.0% | 0.0000 ↓ 100.0% | 0.7134 ↑ 0.4% | 0.7104 ↓ 0.0%\n",
      "Recall@4   | 0.0000 ↓ 100.0% | 0.8811 ↑ 0.4% | 0.0000 ↓ 100.0% | 0.8994 ↑ 2.4% | 0.8780 ↓ 0.0%\n",
      "Recall@6   | 0.0000 ↓ 100.0% | 0.9207 ↑ 2.0% | 0.0000 ↓ 100.0% | 0.9360 ↑ 3.7% | 0.9024 ↓ 0.0%\n",
      "\n",
      "===== Best Methods =====\n",
      "MRR@1: weighted_optimal (0.7134, +14.1%)\n",
      "MRR@4: weighted_optimal (0.7932, +1.8%)\n",
      "MRR@6: weighted_optimal (0.8000, +2.1%)\n",
      "Recall@1: weighted_optimal (0.7134, +0.4%)\n",
      "Recall@4: weighted_optimal (0.8994, +2.4%)\n",
      "Recall@6: weighted_optimal (0.9360, +3.7%)\n",
      "\n",
      "All results saved to hybrid_search_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Run hybrid search evaluation experiments.\"\"\"\n",
    "    print(\"===== Hybrid Search Evaluation =====\")\n",
    "    evaluator = HybridSearchEvaluator('qdrant_documentation_dataset.csv')\n",
    "    \n",
    "    # Evaluate standard hybrid search methods\n",
    "    hybrid_results = evaluator.evaluate_hybrid_search(\n",
    "        hybrid_methods=[\"fusion\", \"dense_rerank\", \"sparse_rerank\"],\n",
    "        k_values=[1, 4, 6]\n",
    "    )\n",
    "    \n",
    "    # Compare with baseline\n",
    "    evaluator.compare_with_baseline(hybrid_results)\n",
    "    \n",
    "    # Find optimal weights with grid search\n",
    "    grid_results = evaluator.find_optimal_weights(\n",
    "        k_values=[1, 4, 6],\n",
    "        weight_steps=5\n",
    "    )\n",
    "    \n",
    "    # Evaluate with the best weights\n",
    "    best_dense_weight, best_sparse_weight = grid_results[\"best\"][\"weights\"]\n",
    "    print(\"\\n===== Evaluating with best weights =====\")\n",
    "    \n",
    "    all_metrics = []\n",
    "    k_values = [1, 4, 6]\n",
    "    \n",
    "    # Process all questions with the best weights\n",
    "    for idx, row in tqdm(evaluator.df.iterrows(), total=len(evaluator.df), \n",
    "                       desc=\"Evaluating best weights\"):\n",
    "        question = row['question']\n",
    "        relevant_section = row['section_content']\n",
    "        relevant_id = evaluator.section_id_map[relevant_section]\n",
    "        \n",
    "        # Get search results with optimal weights\n",
    "        search_results = evaluator.hybrid_search.weighted_hybrid_search(\n",
    "            question,\n",
    "            dense_weight=best_dense_weight,\n",
    "            sparse_weight=best_sparse_weight,\n",
    "            limit=max(k_values)\n",
    "        )\n",
    "        \n",
    "        # Check if we got valid results\n",
    "        if not search_results:\n",
    "            # Empty results, count as 0 for all metrics\n",
    "            query_metrics = {f'Recall@{k}': 0.0 for k in k_values}\n",
    "            query_metrics.update({f'MRR@{k}': 0.0 for k in k_values})\n",
    "        else:\n",
    "            # Calculate metrics\n",
    "            retrieved_ids = [result['id'] for result in search_results]\n",
    "            \n",
    "            # Calculate metrics for current query\n",
    "            query_metrics = {}\n",
    "            for k in k_values:\n",
    "                query_metrics[f'Recall@{k}'] = evaluator.metrics_calculator.calculate_recall_at_k(\n",
    "                    [relevant_id], retrieved_ids, k)\n",
    "                query_metrics[f'MRR@{k}'] = evaluator.metrics_calculator.calculate_mrr_at_k(\n",
    "                    [relevant_id], retrieved_ids, k)\n",
    "        \n",
    "        all_metrics.append(query_metrics)\n",
    "    \n",
    "    # Calculate average metrics for best weights\n",
    "    best_weights_metrics = evaluator.metrics_calculator.compute_average_metrics(all_metrics)\n",
    "    \n",
    "    # Add to comparison\n",
    "    hybrid_results['weighted_optimal'] = best_weights_metrics\n",
    "    \n",
    "    # Compare all results\n",
    "    evaluator.compare_with_baseline(hybrid_results)\n",
    "    \n",
    "    # Save results to CSV\n",
    "    save_results_to_csv(hybrid_results, grid_results, \"hybrid_search_results.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edccb5e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd49ebce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
