{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10369649,"sourceType":"datasetVersion","datasetId":6422990}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.metrics import f1_score\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Константы\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nMAX_LENGTH = 128\nBATCH_SIZE = 16\nEPOCHS = 30\nLEARNING_RATE = 2e-5\nWARMUP_STEPS = 100\nPATIENCE = 5\nMAX_PRED_LABELS = 3\n\nclass TopicDataset(Dataset):\n    def __init__(self, questions, labels, tokenizer, max_length):\n        self.questions = list(questions)\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __len__(self):\n        return len(self.questions)\n    \n    def __getitem__(self, idx):\n        question = str(self.questions[idx])\n        \n        encoding = self.tokenizer.encode_plus(\n            question,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n        \n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.FloatTensor(self.labels[idx])\n        }\n\nclass TopicAttention(nn.Module):\n    def __init__(self, hidden_size):\n        super(TopicAttention, self).__init__()\n        self.attention = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size // 2),\n            nn.Tanh(),\n            nn.Linear(hidden_size // 2, 1)\n        )\n    \n    def forward(self, hidden_states):\n        attention_weights = self.attention(hidden_states)\n        attention_weights = torch.softmax(attention_weights, dim=1)\n        attended_output = torch.sum(attention_weights * hidden_states, dim=1)\n        return attended_output\n\nclass CustomLoss(nn.Module):\n    def __init__(self):\n        super(CustomLoss, self).__init__()\n        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n        \n    def forward(self, outputs, targets):\n        # BCE loss\n        bce_loss = self.bce(outputs, targets)\n        \n        # Штраф за количество предсказаний больше MAX_PRED_LABELS\n        pred_probs = torch.sigmoid(outputs)\n        num_preds = torch.sum(pred_probs > 0.5, dim=1)\n        count_penalty = torch.relu(num_preds - MAX_PRED_LABELS) * 0.1\n        \n        # Штраф за отсутствие правильных меток в топ предсказаниях\n        top_k_values, top_k_indices = torch.topk(pred_probs, k=MAX_PRED_LABELS, dim=1)\n        correct_in_top_k = torch.zeros_like(num_preds, dtype=torch.float)\n        \n        for i in range(targets.size(0)):\n            correct_in_top_k[i] = torch.sum(targets[i][top_k_indices[i]] == 1)\n        \n        accuracy_penalty = torch.exp(-correct_in_top_k) * 0.2\n        \n        total_loss = bce_loss.mean() + count_penalty.mean() + accuracy_penalty.mean()\n        return total_loss\n\nclass TopicClassifier(nn.Module):\n    def __init__(self, n_classes):\n        super(TopicClassifier, self).__init__()\n        self.bert = AutoModel.from_pretrained('DeepPavlov/rubert-base-cased')\n        \n        # Заморозка BERT\n        for param in self.bert.parameters():\n            param.requires_grad = False\n        \n        # Разморозка только последних двух слоев\n        for layer in self.bert.encoder.layer[-2:]:\n            for param in layer.parameters():\n                param.requires_grad = True\n        \n        hidden_size = self.bert.config.hidden_size\n        \n        self.attention = TopicAttention(hidden_size)\n        \n        self.classifier = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size),\n            nn.LayerNorm(hidden_size),\n            nn.GELU(),\n            nn.Dropout(0.3),\n            nn.Linear(hidden_size, hidden_size // 2),\n            nn.LayerNorm(hidden_size // 2),\n            nn.GELU(),\n            nn.Dropout(0.3),\n            nn.Linear(hidden_size // 2, n_classes)\n        )\n        \n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            output_hidden_states=True\n        )\n        \n        # Используем выходы последних 4 слоев\n        last_hidden_states = torch.stack(outputs.hidden_states[-4:])\n        last_hidden_states = last_hidden_states.permute(1, 0, 2, 3)\n        batch_size = last_hidden_states.size(0)\n        last_hidden_states = last_hidden_states.reshape(batch_size, -1, outputs.last_hidden_state.size(-1))\n        \n        # Применяем механизм внимания\n        attended_output = self.attention(last_hidden_states)\n        \n        # Классификация\n        logits = self.classifier(attended_output)\n        return logits\n\ndef train_epoch(model, data_loader, optimizer, criterion, device, scheduler=None):\n    model.train()\n    total_loss = 0\n    \n    for batch in data_loader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask)\n        loss = criterion(outputs, labels)\n        \n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        \n        optimizer.step()\n        if scheduler:\n            scheduler.step()\n        \n        total_loss += loss.item()\n    \n    return total_loss / len(data_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T09:58:08.808609Z","iopub.execute_input":"2025-01-05T09:58:08.808968Z","iopub.status.idle":"2025-01-05T09:58:08.825266Z","shell.execute_reply.started":"2025-01-05T09:58:08.808939Z","shell.execute_reply":"2025-01-05T09:58:08.824364Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def evaluate(model, data_loader, criterion, device):\n    model.eval()\n    total_loss = 0\n    all_predictions = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for batch in data_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            \n            outputs = model(input_ids, attention_mask)\n            loss = criterion(outputs, labels)\n            \n            # Получаем top-k предсказаний\n            probs = torch.sigmoid(outputs)\n            top_k_values, top_k_indices = torch.topk(probs, k=MAX_PRED_LABELS, dim=1)\n            \n            # Создаем бинарную матрицу предсказаний\n            batch_predictions = torch.zeros_like(probs)\n            for i in range(batch_predictions.size(0)):\n                batch_predictions[i][top_k_indices[i]] = 1\n            \n            total_loss += loss.item()\n            all_predictions.extend(batch_predictions.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    return total_loss / len(data_loader), np.array(all_predictions), np.array(all_labels)\n\ndef main(df):\n    # Подготовка данных\n    df = df.reset_index(drop=True)\n    \n    # Подготовка меток\n    mlb = MultiLabelBinarizer()\n    labels = mlb.fit_transform(df['name'].apply(lambda x: [x] if isinstance(x, str) else x))\n    \n    # Разделение данных\n    train_idx, test_idx = train_test_split(df.index, test_size=0.2, random_state=42)\n    train_idx, val_idx = train_test_split(train_idx, test_size=0.2, random_state=42)\n    \n    X_train = df.loc[train_idx, 'question']\n    y_train = labels[train_idx]\n    X_val = df.loc[val_idx, 'question']\n    y_val = labels[val_idx]\n    X_test = df.loc[test_idx, 'question']\n    y_test = labels[test_idx]\n    \n    # Токенизатор\n    tokenizer = AutoTokenizer.from_pretrained('DeepPavlov/rubert-base-cased')\n    \n    # Создание датасетов\n    train_dataset = TopicDataset(X_train, y_train, tokenizer, MAX_LENGTH)\n    val_dataset = TopicDataset(X_val, y_val, tokenizer, MAX_LENGTH)\n    test_dataset = TopicDataset(X_test, y_test, tokenizer, MAX_LENGTH)\n    \n    # Создание даталоадеров\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n    \n    # Инициализация модели\n    model = TopicClassifier(len(mlb.classes_))\n    model.to(DEVICE)\n    \n    # Оптимизатор и планировщик\n    optimizer = torch.optim.AdamW(\n        [\n            {\"params\": model.bert.encoder.layer[-2:].parameters(), \"lr\": LEARNING_RATE},\n            {\"params\": model.attention.parameters(), \"lr\": LEARNING_RATE * 2},\n            {\"params\": model.classifier.parameters(), \"lr\": LEARNING_RATE * 2}\n        ],\n        weight_decay=0.01\n    )\n    \n    total_steps = len(train_loader) * EPOCHS\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=WARMUP_STEPS,\n        num_training_steps=total_steps\n    )\n    \n    criterion = CustomLoss()\n    \n    # Обучение с early stopping\n    best_val_loss = float('inf')\n    patience_counter = 0\n    best_model_state = None\n    \n    for epoch in range(EPOCHS):\n        train_loss = train_epoch(model, train_loader, optimizer, criterion, DEVICE, scheduler)\n        val_loss, val_preds, val_true = evaluate(model, val_loader, criterion, DEVICE)\n        \n        print(f'Epoch {epoch + 1}/{EPOCHS}:')\n        print(f'Train Loss: {train_loss:.4f}')\n        print(f'Val Loss: {val_loss:.4f}')\n        \n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            best_model_state = model.state_dict().copy()\n            patience_counter = 0\n        else:\n            patience_counter += 1\n            \n        if patience_counter >= PATIENCE:\n            print(f'Early stopping triggered after epoch {epoch + 1}')\n            break\n    \n    # Загрузка лучшей модели\n    model.load_state_dict(best_model_state)\n    \n    # Получение предсказаний\n    _, train_preds, train_true = evaluate(model, train_loader, criterion, DEVICE)\n    _, val_preds, val_true = evaluate(model, val_loader, criterion, DEVICE)\n    _, test_preds, test_true = evaluate(model, test_loader, criterion, DEVICE)\n    \n    def format_predictions(preds):\n        return [mlb.classes_[np.where(pred == 1)[0]].tolist() for pred in preds]\n    \n    # Создание финального датафрейма\n    results_df = pd.DataFrame({\n        'question': list(X_test),\n        'sample': ['test'] * len(X_test),\n        'text': df.loc[test_idx, 'text'].values,\n        'true_topic': [mlb.classes_[np.where(y)[0]].tolist() for y in test_true],\n        'predicted_topic': format_predictions(test_preds)\n    })\n    \n    train_results = pd.DataFrame({\n        'question': list(X_train),\n        'sample': ['train'] * len(X_train),\n        'text': df.loc[train_idx, 'text'].values,\n        'true_topic': [mlb.classes_[np.where(y)[0]].tolist() for y in train_true],\n        'predicted_topic': format_predictions(train_preds)\n    })\n    \n    val_results = pd.DataFrame({\n        'question': list(X_val),\n        'sample': ['val'] * len(X_val),\n        'text': df.loc[val_idx, 'text'].values,\n        'true_topic': [mlb.classes_[np.where(y)[0]].tolist() for y in val_true],\n        'predicted_topic': format_predictions(val_preds)\n    })\n    \n    final_df = pd.concat([train_results, val_results, results_df], axis=0, ignore_index=True)\n    \n    # Вычисление финальных метрик\n    test_f1 = f1_score(test_true, test_preds, average='macro')\n    print(f\"\\nФинальный F1-score на тестовой выборке: {test_f1:.4f}\")\n    \n    return final_df, model, test_f1\n\ndef analyze_predictions(df):\n    print(\"\\nАнализ предсказаний:\")\n    \n    # Подсчет точности попадания истинного топика в предсказанные\n    def has_correct_topic(row):\n        return any(topic in row['predicted_topic'] for topic in row['true_topic'])\n    \n    df['has_correct'] = df.apply(has_correct_topic, axis=1)\n    accuracy = df['has_correct'].mean()\n    print(f\"\\nТочность попадания истинного топика в предсказанные: {accuracy:.4f}\")\n    \n    print(\"\\nРаспределение количества предсказанных топиков:\")\n    df['num_predicted_topics'] = df['predicted_topic'].apply(len)\n    print(df.groupby('sample')['num_predicted_topics'].describe())\n    \n    print(\"\\nПримеры предсказаний:\")\n    for sample_type in ['train', 'val', 'test']:\n        print(f\"\\n{sample_type.upper()} примеры:\")\n        sample_predictions = df[df['sample'] == sample_type].sample(min(3, len(df[df['sample'] == sample_type])))\n        for _, row in sample_predictions.iterrows():\n            print(f\"\\nВопрос: {row['question']}\")\n            print(f\"Истинные топики: {row['true_topic']}\")\n            print(f\"Предсказанные топики: {row['predicted_topic']}\")\n            print(f\"Правильное предсказание: {'Да' if row['has_correct'] else 'Нет'}\")\n\nif __name__ == \"__main__\":\n    df = pd.read_csv('/kaggle/input/texts-with-answers/texts_with_answers.csv')\n    results_df, model, test_f1 = main(df)\n    analyze_predictions(results_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T09:58:38.860549Z","iopub.execute_input":"2025-01-05T09:58:38.860909Z","iopub.status.idle":"2025-01-05T09:58:55.051799Z","shell.execute_reply.started":"2025-01-05T09:58:38.860881Z","shell.execute_reply":"2025-01-05T09:58:55.051031Z"}},"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/30:\nTrain Loss: 1.0854\nVal Loss: 1.0509\nEpoch 2/30:\nTrain Loss: 1.0785\nVal Loss: 0.9221\nEpoch 3/30:\nTrain Loss: 1.0297\nVal Loss: 0.8537\nEpoch 4/30:\nTrain Loss: 0.9374\nVal Loss: 0.7658\nEpoch 5/30:\nTrain Loss: 0.8319\nVal Loss: 0.7159\nEpoch 6/30:\nTrain Loss: 0.7554\nVal Loss: 0.6590\nEpoch 7/30:\nTrain Loss: 0.6798\nVal Loss: 0.6012\nEpoch 8/30:\nTrain Loss: 0.6482\nVal Loss: 0.5607\nEpoch 9/30:\nTrain Loss: 0.5944\nVal Loss: 0.5097\nEpoch 10/30:\nTrain Loss: 0.5625\nVal Loss: 0.4903\nEpoch 11/30:\nTrain Loss: 0.5306\nVal Loss: 0.4816\nEpoch 12/30:\nTrain Loss: 0.5242\nVal Loss: 0.4725\nEpoch 13/30:\nTrain Loss: 0.5038\nVal Loss: 0.4667\nEpoch 14/30:\nTrain Loss: 0.4861\nVal Loss: 0.4593\nEpoch 15/30:\nTrain Loss: 0.4818\nVal Loss: 0.4530\nEpoch 16/30:\nTrain Loss: 0.4881\nVal Loss: 0.4518\nEpoch 17/30:\nTrain Loss: 0.4715\nVal Loss: 0.4508\nEpoch 18/30:\nTrain Loss: 0.4560\nVal Loss: 0.4455\nEpoch 19/30:\nTrain Loss: 0.4667\nVal Loss: 0.4517\nEpoch 20/30:\nTrain Loss: 0.4559\nVal Loss: 0.4421\nEpoch 21/30:\nTrain Loss: 0.4434\nVal Loss: 0.4417\nEpoch 22/30:\nTrain Loss: 0.4525\nVal Loss: 0.4421\nEpoch 23/30:\nTrain Loss: 0.4414\nVal Loss: 0.4385\nEpoch 24/30:\nTrain Loss: 0.4440\nVal Loss: 0.4370\nEpoch 25/30:\nTrain Loss: 0.4272\nVal Loss: 0.4364\nEpoch 26/30:\nTrain Loss: 0.4190\nVal Loss: 0.4351\nEpoch 27/30:\nTrain Loss: 0.4159\nVal Loss: 0.4344\nEpoch 28/30:\nTrain Loss: 0.4125\nVal Loss: 0.4340\nEpoch 29/30:\nTrain Loss: 0.4141\nVal Loss: 0.4337\nEpoch 30/30:\nTrain Loss: 0.3996\nVal Loss: 0.4337\n\nФинальный F1-score на тестовой выборке: 0.1727\n\nАнализ предсказаний:\n\nТочность попадания истинного топика в предсказанные: 0.7167\n\nРаспределение количества предсказанных топиков:\n        count  mean  std  min  25%  50%  75%  max\nsample                                           \ntest     24.0   3.0  0.0  3.0  3.0  3.0  3.0  3.0\ntrain    76.0   3.0  0.0  3.0  3.0  3.0  3.0  3.0\nval      20.0   3.0  0.0  3.0  3.0  3.0  3.0  3.0\n\nПримеры предсказаний:\n\nTRAIN примеры:\n\nВопрос: How does Qdrant handle deleting vectors?\nИстинные топики: ['filtering']\nПредсказанные топики: ['filtering', 'hybrid-queries', 'vectors']\nПравильное предсказание: Да\n\nВопрос: How can you modify a point in Qdrant?\nИстинные топики: ['points']\nПредсказанные топики: ['hybrid-queries', 'points', 'vectors']\nПравильное предсказание: Да\n\nВопрос: What type of indexing does Qdrant allow for payload fields to improve search efficiency?\nИстинные топики: ['hybrid-queries']\nПредсказанные топики: ['filtering', 'hybrid-queries', 'vectors']\nПравильное предсказание: Да\n\nVAL примеры:\n\nВопрос: What parameter is used to configure memmap storage for vectors during collection creation?\nИстинные топики: ['storage']\nПредсказанные топики: ['collections', 'explore', 'vectors']\nПравильное предсказание: Нет\n\nВопрос: What is the difference between dense vectors and sparse vectors?\nИстинные топики: ['vectors']\nПредсказанные топики: ['collections', 'snapshots', 'vectors']\nПравильное предсказание: Да\n\nВопрос: What is the default scoring metric for sparse queries in Qdrant?\nИстинные топики: ['search']\nПредсказанные топики: ['hybrid-queries', 'search', 'vectors']\nПравильное предсказание: Да\n\nTEST примеры:\n\nВопрос: What is the significance of using a context in Discovery search?\nИстинные топики: ['explore']\nПредсказанные топики: ['hybrid-queries', 'snapshots', 'vectors']\nПравильное предсказание: Нет\n\nВопрос: What condition would you use to check if a field has multiple values in Qdrant?\nИстинные топики: ['filtering']\nПредсказанные топики: ['hybrid-queries', 'snapshots', 'vectors']\nПравильное предсказание: Нет\n\nВопрос: How can you reference a point ID from a different collection in Qdrant?\nИстинные топики: ['hybrid-queries']\nПредсказанные топики: ['hybrid-queries', 'search', 'vectors']\nПравильное предсказание: Да\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"len(results_df[(results_df['sample'] == 'test') &\n            (results_df['has_correct'] == True)]) / len(results_df[(results_df['sample'] == 'test')])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T10:04:55.200624Z","iopub.execute_input":"2025-01-05T10:04:55.200927Z","iopub.status.idle":"2025-01-05T10:04:55.207712Z","shell.execute_reply.started":"2025-01-05T10:04:55.200903Z","shell.execute_reply":"2025-01-05T10:04:55.207051Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"0.4583333333333333"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"len(results_df[(results_df['sample'] == 'train') &\n            (results_df['has_correct'] == True)]) / len(results_df[(results_df['sample'] == 'train')])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T10:05:09.058115Z","iopub.execute_input":"2025-01-05T10:05:09.058403Z","iopub.status.idle":"2025-01-05T10:05:09.065265Z","shell.execute_reply.started":"2025-01-05T10:05:09.058382Z","shell.execute_reply":"2025-01-05T10:05:09.064382Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"0.8552631578947368"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"len(results_df[(results_df['sample'] == 'val') &\n            (results_df['has_correct'] == True)]) / len(results_df[(results_df['sample'] == 'val')])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T10:05:17.375764Z","iopub.execute_input":"2025-01-05T10:05:17.376075Z","iopub.status.idle":"2025-01-05T10:05:17.382889Z","shell.execute_reply.started":"2025-01-05T10:05:17.376049Z","shell.execute_reply":"2025-01-05T10:05:17.382098Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"0.5"},"metadata":{}}],"execution_count":23}]}